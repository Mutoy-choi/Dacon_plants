{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mutoy-choi/Dacon_plants/blob/main/Gemma/Finetune_with_LLaMA_Factory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2024 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfsDR_omdNea"
      },
      "source": [
        "# Gemma - finetune with LLaMA Factory\n",
        "\n",
        "This notebook demonstrates how to finetune Gemma with LLaMA Factory. [LLaMA Factory](https://github.com/InternLM/xtuner) is a tool that specifically designed for finetuning LLMs. LLaMA Factory wraps the Hugging Face finetuning functionality and provides a simple interface for finetuning. It's very easy to finetune Gemma with LLaMA Factory. This notebook follows very closely the official [Colab notebook](https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing) from LLaMA Factory.\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Gemma/Finetune_with_LLaMA_Factory.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwMiP7jDdAL1"
      },
      "source": [
        "## Setup\n",
        "\n",
        "### Select the Colab runtime\n",
        "To complete this tutorial, you'll need to have a Colab runtime with sufficient resources to run the Gemma model. In this case, you can use a T4 GPU:\n",
        "\n",
        "1. In the upper-right of the Colab window, select **▾ (Additional connection options)**.\n",
        "2. Select **Change runtime type**.\n",
        "3. Under **Hardware accelerator**, select **T4 GPU**.\n",
        "\n",
        "\n",
        "### Gemma setup on Hugging Face\n",
        "LLaMA Factory uses Hugging Face under the hood. So you will need to:\n",
        "\n",
        "* Get access to Gemma on [huggingface.co](huggingface.co) by accepting the Gemma license on the Hugging Face page of the specific model, i.e., [Gemma 2B](https://huggingface.co/google/gemma-2b).\n",
        "* Generate a [Hugging Face access token](https://huggingface.co/docs/hub/en/security-tokens) and configure it as a Colab secret 'HF_TOKEN'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AVvJYwne3hha"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "# Note: `userdata.get` is a Colab API. If you're not using Colab, set the env\n",
        "# vars as appropriate for your system.\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yUF4Hk5dOoz"
      },
      "source": [
        "### Install LLaMA Factory\n",
        "\n",
        "Install LLaMA Factory from source on GitHub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4pY14h6_bDrr",
        "collapsed": true,
        "outputId": "8bb24c57-ca4d-490c-f2b4-21c74c30c523",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 20658, done.\u001b[K\n",
            "remote: Counting objects: 100% (275/275), done.\u001b[K\n",
            "remote: Compressing objects: 100% (110/110), done.\u001b[K\n",
            "remote: Total 20658 (delta 215), reused 165 (delta 165), pack-reused 20383 (from 3)\u001b[K\n",
            "Receiving objects: 100% (20658/20658), 235.50 MiB | 17.16 MiB/s, done.\n",
            "Resolving deltas: 100% (14935/14935), done.\n",
            "/content/LLaMA-Factory\n",
            "\u001b[0m\u001b[01;34massets\u001b[0m/       \u001b[01;34mdocker\u001b[0m/      LICENSE      pyproject.toml  requirements.txt  \u001b[01;34msrc\u001b[0m/\n",
            "CITATION.cff  \u001b[01;34mevaluation\u001b[0m/  Makefile     README.md       \u001b[01;34mscripts\u001b[0m/          \u001b[01;34mtests\u001b[0m/\n",
            "\u001b[01;34mdata\u001b[0m/         \u001b[01;34mexamples\u001b[0m/    MANIFEST.in  README_zh.md    setup.py\n",
            "Obtaining file:///content/LLaMA-Factory\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers<=4.46.1,>=4.41.2 (from llamafactory==0.9.2.dev0)\n",
            "  Downloading transformers-4.46.1-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets<=3.1.0,>=2.16.0 (from llamafactory==0.9.2.dev0)\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting accelerate<=1.0.1,>=0.34.0 (from llamafactory==0.9.2.dev0)\n",
            "  Downloading accelerate-1.0.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting peft<=0.12.0,>=0.11.1 (from llamafactory==0.9.2.dev0)\n",
            "  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting trl<=0.9.6,>=0.8.6 (from llamafactory==0.9.2.dev0)\n",
            "  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting tokenizers<0.20.4,>=0.19.0 (from llamafactory==0.9.2.dev0)\n",
            "  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting gradio<5.0.0,>=4.0.0 (from llamafactory==0.9.2.dev0)\n",
            "  Downloading gradio-4.44.1-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (1.13.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (0.8.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (0.2.0)\n",
            "Collecting tiktoken (from llamafactory==0.9.2.dev0)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (4.25.5)\n",
            "Collecting uvicorn (from llamafactory==0.9.2.dev0)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (2.10.3)\n",
            "Collecting fastapi (from llamafactory==0.9.2.dev0)\n",
            "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting sse-starlette (from llamafactory==0.9.2.dev0)\n",
            "  Downloading sse_starlette-2.2.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (3.8.0)\n",
            "Collecting fire (from llamafactory==0.9.2.dev0)\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (6.0.2)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (1.26.4)\n",
            "Collecting av (from llamafactory==0.9.2.dev0)\n",
            "  Downloading av-14.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting tyro<0.9.0 (from llamafactory==0.9.2.dev0)\n",
            "  Downloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting bitsandbytes>=0.39.0 (from llamafactory==0.9.2.dev0)\n",
            "  Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (2.5.1+cu121)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.0.1,>=0.34.0->llamafactory==0.9.2.dev0) (5.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.0.1,>=0.34.0->llamafactory==0.9.2.dev0) (0.27.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.0.1,>=0.34.0->llamafactory==0.9.2.dev0) (0.4.5)\n",
            "Requirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes>=0.39.0->llamafactory==0.9.2.dev0) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (4.67.1)\n",
            "Collecting xxhash (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (3.11.10)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (3.7.1)\n",
            "Collecting ffmpy (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.3.0 (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
            "  Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (0.28.1)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (6.4.5)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (3.1.4)\n",
            "Collecting markupsafe~=2.0 (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (3.10.12)\n",
            "Collecting pillow<11.0,>=8.0 (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
            "  Downloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting pydub (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting ruff>=0.2.2 (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
            "  Downloading ruff-0.9.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (0.15.1)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (2.2.3)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.3.0->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting starlette<0.42.0,>=0.40.0 (from fastapi->llamafactory==0.9.2.dev0)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.9.2.dev0) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.9.2.dev0) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.9.2.dev0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.9.2.dev0) (2.27.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (3.4.2)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.1->llamafactory==0.9.2.dev0) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<=4.46.1,>=4.41.2->llamafactory==0.9.2.dev0) (2024.11.6)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0->llamafactory==0.9.2.dev0) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0->llamafactory==0.9.2.dev0) (13.9.4)\n",
            "Collecting shtab>=1.5.6 (from tyro<0.9.0->llamafactory==0.9.2.dev0)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.9.2.dev0) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.9.2.dev0) (0.14.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.9.2.dev0) (2.5.0)\n",
            "Collecting anyio<5.0,>=3.0 (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0)\n",
            "  Downloading anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (1.2.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (1.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (1.18.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (1.0.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (3.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.2.dev0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.2.dev0) (2.18.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (1.5.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.2.dev0) (0.1.2)\n",
            "Downloading accelerate-1.0.1-py3-none-any.whl (330 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.9/330.9 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl (69.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-4.44.1-py3-none-any.whl (18.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.46.1-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.8.14-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading av-14.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.0/33.0 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sse_starlette-2.2.1-py3-none-any.whl (10 kB)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading anyio-4.8.0-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.0/96.0 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.9.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llamafactory, fire\n",
            "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llamafactory: filename=llamafactory-0.9.2.dev0-0.editable-py3-none-any.whl size=25151 sha256=8a2e55fe8bdba58988f131f2edfcda62c1d1f77145529be1759d0a45c6575f46\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wee2pfsk/wheels/de/aa/c5/27b5682c5592b7c0eecc3e208f176dedf6b11a61cf2a910b85\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=343d07efb7c39cc20e2853031aa006ed5a03ae7a9efebd5e4942ff004d2bc353\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/39/2f/2d3cadc408a8804103f1c34ddd4b9f6a93497b11fa96fe738e\n",
            "Successfully built llamafactory fire\n",
            "Installing collected packages: pydub, xxhash, websockets, uvicorn, tomlkit, shtab, semantic-version, ruff, python-multipart, pillow, markupsafe, fsspec, fire, ffmpy, dill, av, anyio, aiofiles, tiktoken, starlette, multiprocess, tyro, tokenizers, sse-starlette, gradio-client, fastapi, transformers, gradio, bitsandbytes, accelerate, peft, datasets, trl, llamafactory\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 14.1\n",
            "    Uninstalling websockets-14.1:\n",
            "      Successfully uninstalled websockets-14.1\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.0.0\n",
            "    Uninstalling pillow-11.0.0:\n",
            "      Successfully uninstalled pillow-11.0.0\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "  Attempting uninstall: anyio\n",
            "    Found existing installation: anyio 3.7.1\n",
            "    Uninstalling anyio-3.7.1:\n",
            "      Successfully uninstalled anyio-3.7.1\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.0\n",
            "    Uninstalling tokenizers-0.21.0:\n",
            "      Successfully uninstalled tokenizers-0.21.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.47.1\n",
            "    Uninstalling transformers-4.47.1:\n",
            "      Successfully uninstalled transformers-4.47.1\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.2.1\n",
            "    Uninstalling accelerate-1.2.1:\n",
            "      Successfully uninstalled accelerate-1.2.1\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.14.0\n",
            "    Uninstalling peft-0.14.0:\n",
            "      Successfully uninstalled peft-0.14.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\n",
            "google-genai 0.3.0 requires websockets<15.0dev,>=13.0, but you have websockets 12.0 which is incompatible.\n",
            "jupyter-server 1.24.0 requires anyio<4,>=3.1.0, but you have anyio 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-1.0.1 aiofiles-23.2.1 anyio-4.8.0 av-14.0.1 bitsandbytes-0.45.0 datasets-3.1.0 dill-0.3.8 fastapi-0.115.6 ffmpy-0.5.0 fire-0.7.0 fsspec-2024.9.0 gradio-4.44.1 gradio-client-1.3.0 llamafactory-0.9.2.dev0 markupsafe-2.1.5 multiprocess-0.70.16 peft-0.12.0 pillow-10.4.0 pydub-0.25.1 python-multipart-0.0.20 ruff-0.9.1 semantic-version-2.10.0 shtab-1.7.1 sse-starlette-2.2.1 starlette-0.41.3 tiktoken-0.8.0 tokenizers-0.20.3 tomlkit-0.12.0 transformers-4.46.1 trl-0.9.6 tyro-0.8.14 uvicorn-0.34.0 websockets-12.0 xxhash-3.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              },
              "id": "67f826a24a504a7bb94928256e85b8aa"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "%cd /content/\n",
        "%rm -rf LLaMA-Factory\n",
        "!git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory\n",
        "%ls\n",
        "!pip install -e .[torch,bitsandbytes]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Di9D2DY5dqmw"
      },
      "source": [
        "## Finetune Gemma\n",
        "\n",
        "Kick off Gemma 2B finetuning with a [demo Alpaca dataset](https://github.com/hiyouga/LLaMA-Factory/blob/main/data/alpaca_en_demo.json). If you want to use your own dataset, follow this [guide from LLaMA Factory](https://github.com/hiyouga/LLaMA-Factory/tree/main/data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gWIzVxhwcDSw",
        "outputId": "0e4cf9d2-bf72-483d-8123-46e50db1ed1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "2025-01-15 11:56:06.635800: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-15 11:56:06.655872: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-15 11:56:06.661652: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-15 11:56:06.678003: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-01-15 11:56:08.185643: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[WARNING|2025-01-15 11:56:14] llamafactory.hparams.parser:162 >> We recommend enable `upcast_layernorm` in quantized training.\n",
            "[INFO|2025-01-15 11:56:14] llamafactory.hparams.parser:380 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "config.json: 100% 627/627 [00:00<00:00, 3.91MB/s]\n",
            "[INFO|configuration_utils.py:679] 2025-01-15 11:56:14,791 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\n",
            "[INFO|configuration_utils.py:746] 2025-01-15 11:56:14,793 >> Model config GemmaConfig {\n",
            "  \"_name_or_path\": \"google/gemma-2b-it\",\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "tokenizer_config.json: 100% 34.2k/34.2k [00:00<00:00, 24.2MB/s]\n",
            "tokenizer.model: 100% 4.24M/4.24M [00:00<00:00, 40.9MB/s]\n",
            "tokenizer.json: 100% 17.5M/17.5M [00:00<00:00, 41.4MB/s]\n",
            "special_tokens_map.json: 100% 636/636 [00:00<00:00, 4.30MB/s]\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-15 11:56:17,769 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-15 11:56:17,769 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-15 11:56:17,769 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-15 11:56:17,769 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-15 11:56:17,770 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:679] 2025-01-15 11:56:20,209 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\n",
            "[INFO|configuration_utils.py:746] 2025-01-15 11:56:20,211 >> Model config GemmaConfig {\n",
            "  \"_name_or_path\": \"google/gemma-2b-it\",\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-15 11:56:20,447 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-15 11:56:20,447 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-15 11:56:20,447 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-15 11:56:20,447 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-15 11:56:20,447 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer_config.json\n",
            "[INFO|2025-01-15 11:56:22] llamafactory.data.loader:157 >> Loading dataset train_transformed.json...\n",
            "Running tokenizer on dataset: 100% 500/500 [00:00<00:00, 1731.21 examples/s]\n",
            "training example:\n",
            "input_ids:\n",
            "[2, 106, 1645, 108, 67556, 573, 2412, 5248, 11070, 49748, 2793, 577, 1277, 3464, 6996, 235265, 108, 239793, 35191, 85181, 238772, 235248, 239985, 247872, 249576, 235265, 235248, 242940, 235248, 242686, 238912, 243483, 240356, 235248, 246544, 235248, 235274, 240457, 236791, 235248, 252902, 241105, 241330, 235248, 453, 404, 382, 250816, 241562, 235248, 242686, 238912, 253978, 243262, 247539, 235248, 452, 384, 363, 238779, 235248, 453, 349, 384, 245334, 254670, 250036, 244604, 235248, 253982, 452, 346, 359, 452, 380, 361, 241105, 235248, 239228, 242780, 235248, 250232, 237905, 955, 137965, 452, 390, 358, 238035, 235248, 246715, 75980, 235248, 452, 356, 403, 241215, 235248, 251905, 248054, 235248, 246799, 241573, 235248, 451, 398, 380, 235265, 235248, 451, 402, 401, 453, 347, 382, 255940, 235248, 235284, 235276, 237386, 84649, 235248, 452, 357, 373, 245029, 452, 398, 385, 235248, 244315, 47250, 235248, 453, 386, 385, 245178, 235248, 451, 400, 362, 246697, 235248, 243622, 253681, 242251, 235248, 251032, 235265, 107, 108, 106, 2516, 108, 239793, 35191, 49532, 236840, 23745, 247872, 236039, 235265, 235248, 242940, 64691, 94525, 211017, 235248, 235274, 237936, 236791, 40712, 236214, 236183, 235248, 246393, 236770, 238744, 64691, 26291, 236554, 72163, 236375, 220596, 236345, 236645, 236405, 235248, 230019, 236375, 236214, 162056, 237908, 159897, 236464, 955, 23745, 237908, 241184, 93828, 84389, 171167, 23248, 240373, 235248, 246799, 236648, 226822, 235265, 235248, 242968, 241496, 236392, 235248, 235284, 235276, 237386, 84649, 32048, 239228, 238744, 226822, 47250, 31850, 236666, 28693, 238304, 38585, 254248, 240080, 226822, 235265, 107, 108]\n",
            "inputs:\n",
            "<bos><start_of_turn>user\n",
            "Restore the following phonetically encoded text to its original meaning.\n",
            "별 한 게토 았깝땀. 왜 싸람듯릭 펼 1캐를 쥰눈징 컥꺾폰 싸람믐롯섞 맒록 섧멍핥쟈닐 탯끎룐눈 녀뮤 퀼교... 야뭍툰 둠 변 닺씨 깍낄 싫훈 굣. 깸삥읊 20여 년 댜녁뵨 곧 중 쩨윌 귑푼 낙팠떤 곶.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "별 한 개도 아깝다. 왜 사람들이 별 1개를 주는지 겪어본 사람으로서 말로 설명하자니 댓글로는 너무 길고... 아무튼 두 번 다시 가길 싫은 곳. 캠핑을 20여 년 다녀본 곳 중 제일 기분 나빴던 곳.<end_of_turn>\n",
            "\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 239793, 35191, 49532, 236840, 23745, 247872, 236039, 235265, 235248, 242940, 64691, 94525, 211017, 235248, 235274, 237936, 236791, 40712, 236214, 236183, 235248, 246393, 236770, 238744, 64691, 26291, 236554, 72163, 236375, 220596, 236345, 236645, 236405, 235248, 230019, 236375, 236214, 162056, 237908, 159897, 236464, 955, 23745, 237908, 241184, 93828, 84389, 171167, 23248, 240373, 235248, 246799, 236648, 226822, 235265, 235248, 242968, 241496, 236392, 235248, 235284, 235276, 237386, 84649, 32048, 239228, 238744, 226822, 47250, 31850, 236666, 28693, 238304, 38585, 254248, 240080, 226822, 235265, 107, 108]\n",
            "labels:\n",
            "별 한 개도 아깝다. 왜 사람들이 별 1개를 주는지 겪어본 사람으로서 말로 설명하자니 댓글로는 너무 길고... 아무튼 두 번 다시 가길 싫은 곳. 캠핑을 20여 년 다녀본 곳 중 제일 기분 나빴던 곳.<end_of_turn>\n",
            "\n",
            "[INFO|configuration_utils.py:679] 2025-01-15 11:56:24,111 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\n",
            "[INFO|configuration_utils.py:746] 2025-01-15 11:56:24,112 >> Model config GemmaConfig {\n",
            "  \"_name_or_path\": \"google/gemma-2b-it\",\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|2025-01-15 11:56:24] llamafactory.model.model_utils.quantization:157 >> Quantizing model to 4 bit with bitsandbytes.\n",
            "model.safetensors.index.json: 100% 13.5k/13.5k [00:00<00:00, 53.5MB/s]\n",
            "[INFO|modeling_utils.py:3937] 2025-01-15 11:56:25,876 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/model.safetensors.index.json\n",
            "Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00001-of-00002.safetensors:   0% 0.00/4.95G [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 10.5M/4.95G [00:00<01:57, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 21.0M/4.95G [00:00<01:55, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 31.5M/4.95G [00:00<01:56, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 41.9M/4.95G [00:01<01:57, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 52.4M/4.95G [00:01<01:57, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 62.9M/4.95G [00:01<01:57, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 73.4M/4.95G [00:01<01:56, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 83.9M/4.95G [00:02<01:56, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 94.4M/4.95G [00:02<01:55, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 105M/4.95G [00:02<01:55, 41.8MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 115M/4.95G [00:02<01:56, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 126M/4.95G [00:03<01:56, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 136M/4.95G [00:03<01:56, 41.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 147M/4.95G [00:03<01:54, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 157M/4.95G [00:03<01:56, 41.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 168M/4.95G [00:04<01:55, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 178M/4.95G [00:04<01:54, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 189M/4.95G [00:04<01:54, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 199M/4.95G [00:04<01:53, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 210M/4.95G [00:05<01:54, 41.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 220M/4.95G [00:05<01:53, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 231M/4.95G [00:05<01:52, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 241M/4.95G [00:05<01:53, 41.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 252M/4.95G [00:06<01:53, 41.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 262M/4.95G [00:06<01:56, 40.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 273M/4.95G [00:06<01:54, 40.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 283M/4.95G [00:06<01:52, 41.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 294M/4.95G [00:07<01:52, 41.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 304M/4.95G [00:07<01:51, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 315M/4.95G [00:07<01:51, 41.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 325M/4.95G [00:07<01:51, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 336M/4.95G [00:08<01:52, 41.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 346M/4.95G [00:08<01:50, 41.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 357M/4.95G [00:08<01:50, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 367M/4.95G [00:08<01:49, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 377M/4.95G [00:09<01:49, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 388M/4.95G [00:09<01:50, 41.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 398M/4.95G [00:09<01:49, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 409M/4.95G [00:09<01:48, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 419M/4.95G [00:10<01:48, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 430M/4.95G [00:10<01:47, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 440M/4.95G [00:10<01:47, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 451M/4.95G [00:10<01:47, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 461M/4.95G [00:11<01:47, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 472M/4.95G [00:11<01:46, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 482M/4.95G [00:11<01:46, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 493M/4.95G [00:11<01:45, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 503M/4.95G [00:12<01:45, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 514M/4.95G [00:12<01:44, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 524M/4.95G [00:12<01:44, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 535M/4.95G [00:12<01:44, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 545M/4.95G [00:13<01:44, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 556M/4.95G [00:13<01:44, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 566M/4.95G [00:13<01:44, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 577M/4.95G [00:13<01:44, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 587M/4.95G [00:14<01:43, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 598M/4.95G [00:14<01:44, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 608M/4.95G [00:14<01:44, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 619M/4.95G [00:14<01:43, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 629M/4.95G [00:15<01:43, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 640M/4.95G [00:15<01:42, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 650M/4.95G [00:15<01:41, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 661M/4.95G [00:15<01:41, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 671M/4.95G [00:16<01:43, 41.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 682M/4.95G [00:16<01:43, 41.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 692M/4.95G [00:16<01:42, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 703M/4.95G [00:16<01:42, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 713M/4.95G [00:17<01:41, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 724M/4.95G [00:17<01:41, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 734M/4.95G [00:17<01:40, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 744M/4.95G [00:17<01:40, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 755M/4.95G [00:18<01:39, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 765M/4.95G [00:18<01:39, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 776M/4.95G [00:18<01:39, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 786M/4.95G [00:18<01:39, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 797M/4.95G [00:19<01:39, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 807M/4.95G [00:19<01:38, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 818M/4.95G [00:19<01:38, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 828M/4.95G [00:19<01:38, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 839M/4.95G [00:20<01:37, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 849M/4.95G [00:20<01:38, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 860M/4.95G [00:20<01:36, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 870M/4.95G [00:20<01:36, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 881M/4.95G [00:21<01:36, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 891M/4.95G [00:21<01:36, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 902M/4.95G [00:21<01:35, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 912M/4.95G [00:21<01:35, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 923M/4.95G [00:22<01:34, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 933M/4.95G [00:22<01:30, 44.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 954M/4.95G [00:22<01:13, 54.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 965M/4.95G [00:22<01:18, 50.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 975M/4.95G [00:23<01:22, 48.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 986M/4.95G [00:23<01:24, 46.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 996M/4.95G [00:23<01:27, 45.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 1.01G/4.95G [00:23<01:28, 44.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.02G/4.95G [00:24<01:29, 43.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.03G/4.95G [00:24<01:32, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.04G/4.95G [00:24<01:32, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.05G/4.95G [00:24<01:35, 40.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.06G/4.95G [00:25<01:35, 40.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.07G/4.95G [00:25<01:34, 41.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.08G/4.95G [00:25<01:32, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.09G/4.95G [00:25<01:32, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.10G/4.95G [00:26<01:34, 40.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.11G/4.95G [00:26<01:33, 40.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.12G/4.95G [00:26<01:33, 41.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.13G/4.95G [00:26<01:32, 41.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.14G/4.95G [00:27<01:31, 41.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.15G/4.95G [00:27<01:32, 41.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.16G/4.95G [00:27<01:31, 41.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.17G/4.95G [00:27<01:30, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.18G/4.95G [00:28<01:29, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.20G/4.95G [00:28<01:29, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.21G/4.95G [00:28<01:28, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.22G/4.95G [00:28<01:28, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.23G/4.95G [00:29<01:28, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.24G/4.95G [00:29<01:28, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.25G/4.95G [00:29<01:27, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.26G/4.95G [00:29<01:27, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.27G/4.95G [00:30<01:26, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.28G/4.95G [00:30<01:26, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.29G/4.95G [00:30<01:26, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.30G/4.95G [00:30<01:26, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.31G/4.95G [00:31<01:25, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.32G/4.95G [00:31<01:25, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.33G/4.95G [00:31<01:25, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.34G/4.95G [00:31<01:25, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.35G/4.95G [00:32<01:24, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.36G/4.95G [00:32<01:40, 35.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.37G/4.95G [00:32<01:31, 39.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.38G/4.95G [00:32<01:29, 39.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.39G/4.95G [00:33<01:27, 40.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.41G/4.95G [00:33<01:25, 41.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.42G/4.95G [00:33<01:25, 41.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.43G/4.95G [00:33<01:24, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.44G/4.95G [00:34<01:24, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.45G/4.95G [00:34<01:23, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.46G/4.95G [00:34<01:23, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.47G/4.95G [00:34<01:23, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.48G/4.95G [00:35<01:23, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.49G/4.95G [00:35<01:22, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.50G/4.95G [00:35<01:21, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.51G/4.95G [00:35<01:22, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.52G/4.95G [00:36<01:21, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.53G/4.95G [00:36<01:21, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.54G/4.95G [00:36<01:20, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.55G/4.95G [00:36<01:20, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.56G/4.95G [00:37<01:21, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.57G/4.95G [00:37<01:20, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.58G/4.95G [00:37<01:20, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.59G/4.95G [00:37<01:20, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.60G/4.95G [00:38<01:19, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.61G/4.95G [00:38<01:19, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.63G/4.95G [00:38<01:18, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.64G/4.95G [00:38<01:18, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.65G/4.95G [00:39<01:18, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.66G/4.95G [00:39<01:18, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.67G/4.95G [00:39<01:18, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.68G/4.95G [00:39<01:18, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.69G/4.95G [00:40<01:18, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.70G/4.95G [00:40<01:17, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.71G/4.95G [00:40<01:17, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.72G/4.95G [00:40<01:16, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.73G/4.95G [00:41<01:15, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.74G/4.95G [00:41<01:16, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.75G/4.95G [00:41<01:16, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.76G/4.95G [00:41<01:15, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.77G/4.95G [00:42<01:15, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.78G/4.95G [00:42<01:15, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.79G/4.95G [00:42<01:14, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.80G/4.95G [00:42<01:02, 50.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.81G/4.95G [00:43<01:00, 52.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.82G/4.95G [00:43<01:07, 46.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.84G/4.95G [00:43<01:09, 44.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.85G/4.95G [00:43<01:10, 43.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.86G/4.95G [00:44<01:11, 43.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.87G/4.95G [00:44<01:11, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.88G/4.95G [00:44<01:11, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.89G/4.95G [00:44<01:11, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.90G/4.95G [00:45<01:12, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.91G/4.95G [00:45<01:12, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.92G/4.95G [00:45<01:12, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.93G/4.95G [00:45<01:11, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.94G/4.95G [00:46<01:11, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.95G/4.95G [00:46<01:11, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 1.96G/4.95G [00:46<01:10, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 1.97G/4.95G [00:46<01:10, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 1.98G/4.95G [00:47<01:10, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 1.99G/4.95G [00:47<01:09, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 2.00G/4.95G [00:47<01:09, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.01G/4.95G [00:47<01:08, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.02G/4.95G [00:48<01:08, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.03G/4.95G [00:48<01:08, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.04G/4.95G [00:48<01:08, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.06G/4.95G [00:48<01:08, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.07G/4.95G [00:49<01:08, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.08G/4.95G [00:49<01:08, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.09G/4.95G [00:49<01:09, 41.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.10G/4.95G [00:49<01:08, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.11G/4.95G [00:50<01:07, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.12G/4.95G [00:50<01:07, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.13G/4.95G [00:50<01:07, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.14G/4.95G [00:50<01:06, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.15G/4.95G [00:51<01:06, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.16G/4.95G [00:51<01:06, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.17G/4.95G [00:51<01:06, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.18G/4.95G [00:51<01:06, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.19G/4.95G [00:52<01:06, 41.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.20G/4.95G [00:52<01:06, 41.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.21G/4.95G [00:52<01:05, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.22G/4.95G [00:52<01:05, 41.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.23G/4.95G [00:53<01:04, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.24G/4.95G [00:53<00:56, 48.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.25G/4.95G [00:53<01:03, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.26G/4.95G [00:53<01:03, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.28G/4.95G [00:54<01:04, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.29G/4.95G [00:54<01:03, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.30G/4.95G [00:54<01:03, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.31G/4.95G [00:54<01:03, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.32G/4.95G [00:55<01:03, 41.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.33G/4.95G [00:55<01:02, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.34G/4.95G [00:55<01:02, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.35G/4.95G [00:55<01:02, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.36G/4.95G [00:56<01:02, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.37G/4.95G [00:56<01:02, 41.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.38G/4.95G [00:56<01:02, 41.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.39G/4.95G [00:56<01:01, 41.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.40G/4.95G [00:57<01:01, 41.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.41G/4.95G [00:57<01:00, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.42G/4.95G [00:57<01:00, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.43G/4.95G [00:57<01:00, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.44G/4.95G [00:58<01:00, 41.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.45G/4.95G [00:58<01:00, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.46G/4.95G [00:58<00:59, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.47G/4.95G [00:58<01:00, 40.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.49G/4.95G [00:59<00:59, 41.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.50G/4.95G [00:59<00:58, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.51G/4.95G [00:59<00:58, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.52G/4.95G [00:59<00:58, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.53G/4.95G [01:00<00:57, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.54G/4.95G [01:00<00:57, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.55G/4.95G [01:00<00:57, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.56G/4.95G [01:00<00:56, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.57G/4.95G [01:01<00:56, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.58G/4.95G [01:01<00:56, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.59G/4.95G [01:01<00:56, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.60G/4.95G [01:01<00:56, 41.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.61G/4.95G [01:02<00:56, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.62G/4.95G [01:02<00:55, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.63G/4.95G [01:02<00:54, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.64G/4.95G [01:02<00:54, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.65G/4.95G [01:03<00:54, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.66G/4.95G [01:03<00:53, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.67G/4.95G [01:03<00:52, 43.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.68G/4.95G [01:03<00:54, 41.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.69G/4.95G [01:04<00:53, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.71G/4.95G [01:04<00:53, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.72G/4.95G [01:04<00:53, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.73G/4.95G [01:04<00:53, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.74G/4.95G [01:05<00:53, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.75G/4.95G [01:05<00:52, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.76G/4.95G [01:05<00:53, 41.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.77G/4.95G [01:05<00:52, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.78G/4.95G [01:06<00:52, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.79G/4.95G [01:06<00:51, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.80G/4.95G [01:06<00:51, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.81G/4.95G [01:06<00:50, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.82G/4.95G [01:07<00:50, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.83G/4.95G [01:07<00:50, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.84G/4.95G [01:07<00:49, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.85G/4.95G [01:07<00:49, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.86G/4.95G [01:08<00:50, 41.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.87G/4.95G [01:08<00:50, 41.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.88G/4.95G [01:08<00:49, 41.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.89G/4.95G [01:08<00:49, 41.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.90G/4.95G [01:09<00:53, 38.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.92G/4.95G [01:09<00:54, 36.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.93G/4.95G [01:09<00:52, 38.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.94G/4.95G [01:09<00:51, 39.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.95G/4.95G [01:10<00:50, 39.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.96G/4.95G [01:10<00:50, 39.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.97G/4.95G [01:10<00:49, 39.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.98G/4.95G [01:11<00:51, 38.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.99G/4.95G [01:11<00:49, 39.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.00G/4.95G [01:11<00:48, 40.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.01G/4.95G [01:11<00:48, 40.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.02G/4.95G [01:12<00:51, 37.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.03G/4.95G [01:12<00:49, 38.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.04G/4.95G [01:12<00:49, 38.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.05G/4.95G [01:12<00:48, 39.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.06G/4.95G [01:13<00:47, 40.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.07G/4.95G [01:13<00:46, 40.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.08G/4.95G [01:13<00:45, 41.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.09G/4.95G [01:13<00:46, 40.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.10G/4.95G [01:14<00:45, 40.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.11G/4.95G [01:14<00:44, 41.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.12G/4.95G [01:14<00:44, 41.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.14G/4.95G [01:14<00:44, 40.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.15G/4.95G [01:15<00:43, 41.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.16G/4.95G [01:15<00:43, 41.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.17G/4.95G [01:15<00:45, 39.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.18G/4.95G [01:16<00:44, 39.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.19G/4.95G [01:16<00:44, 39.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.20G/4.95G [01:16<00:44, 39.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.21G/4.95G [01:16<00:44, 39.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.22G/4.95G [01:17<00:43, 40.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.23G/4.95G [01:17<00:42, 40.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.24G/4.95G [01:17<00:41, 41.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.25G/4.95G [01:17<00:41, 41.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.26G/4.95G [01:18<00:40, 41.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.27G/4.95G [01:18<00:40, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.28G/4.95G [01:18<00:39, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.29G/4.95G [01:18<00:39, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.30G/4.95G [01:19<00:39, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.31G/4.95G [01:19<00:38, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.32G/4.95G [01:19<00:38, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.33G/4.95G [01:19<00:39, 41.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.34G/4.95G [01:20<00:41, 38.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.36G/4.95G [01:20<00:40, 38.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.37G/4.95G [01:20<00:39, 39.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.38G/4.95G [01:20<00:39, 40.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.39G/4.95G [01:21<00:38, 40.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.40G/4.95G [01:21<00:37, 41.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.41G/4.95G [01:21<00:37, 41.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.42G/4.95G [01:21<00:36, 41.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.43G/4.95G [01:22<00:36, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.44G/4.95G [01:22<00:36, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.45G/4.95G [01:22<00:36, 41.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.46G/4.95G [01:22<00:36, 41.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.47G/4.95G [01:23<00:35, 41.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.48G/4.95G [01:23<00:35, 41.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.49G/4.95G [01:23<00:33, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.50G/4.95G [01:23<00:31, 45.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.51G/4.95G [01:24<00:32, 44.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.52G/4.95G [01:24<00:32, 43.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.53G/4.95G [01:24<00:32, 43.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.54G/4.95G [01:24<00:32, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.55G/4.95G [01:25<00:32, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.57G/4.95G [01:25<00:32, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.58G/4.95G [01:25<00:32, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.59G/4.95G [01:25<00:32, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.60G/4.95G [01:26<00:32, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.61G/4.95G [01:26<00:31, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.62G/4.95G [01:26<00:31, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.63G/4.95G [01:26<00:31, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.64G/4.95G [01:27<00:30, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.65G/4.95G [01:27<00:30, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.66G/4.95G [01:27<00:30, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.67G/4.95G [01:27<00:30, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.68G/4.95G [01:28<00:29, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.69G/4.95G [01:28<00:29, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.70G/4.95G [01:28<00:29, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.71G/4.95G [01:28<00:29, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.72G/4.95G [01:29<00:31, 38.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.73G/4.95G [01:29<00:30, 39.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.74G/4.95G [01:29<00:29, 40.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.75G/4.95G [01:29<00:29, 39.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.76G/4.95G [01:30<00:29, 40.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.77G/4.95G [01:30<00:28, 41.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.79G/4.95G [01:30<00:28, 41.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.80G/4.95G [01:30<00:27, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.81G/4.95G [01:31<00:27, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.82G/4.95G [01:31<00:26, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.83G/4.95G [01:31<00:26, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.84G/4.95G [01:31<00:26, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.85G/4.95G [01:32<00:25, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.86G/4.95G [01:32<00:25, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.87G/4.95G [01:32<00:25, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.88G/4.95G [01:32<00:25, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.89G/4.95G [01:33<00:24, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.90G/4.95G [01:33<00:24, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.91G/4.95G [01:33<00:24, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.92G/4.95G [01:33<00:24, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.93G/4.95G [01:34<00:22, 45.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.94G/4.95G [01:34<00:22, 44.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.95G/4.95G [01:34<00:22, 44.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.96G/4.95G [01:34<00:22, 43.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.97G/4.95G [01:35<00:22, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 3.98G/4.95G [01:35<00:22, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.00G/4.95G [01:35<00:22, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.01G/4.95G [01:35<00:22, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.02G/4.95G [01:36<00:22, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.03G/4.95G [01:36<00:23, 39.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.04G/4.95G [01:36<00:22, 40.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.05G/4.95G [01:36<00:22, 40.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.06G/4.95G [01:37<00:21, 40.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.07G/4.95G [01:37<00:21, 40.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.08G/4.95G [01:37<00:21, 40.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.09G/4.95G [01:37<00:20, 40.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.10G/4.95G [01:38<00:20, 41.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.11G/4.95G [01:38<00:20, 40.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.12G/4.95G [01:38<00:20, 41.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.13G/4.95G [01:38<00:19, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.14G/4.95G [01:39<00:19, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.15G/4.95G [01:39<00:18, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.16G/4.95G [01:39<00:18, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.17G/4.95G [01:39<00:18, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.18G/4.95G [01:40<00:18, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.19G/4.95G [01:40<00:18, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.20G/4.95G [01:40<00:17, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.22G/4.95G [01:40<00:17, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.23G/4.95G [01:41<00:17, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.24G/4.95G [01:41<00:18, 38.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.25G/4.95G [01:41<00:21, 32.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.26G/4.95G [01:42<00:20, 34.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.27G/4.95G [01:42<00:18, 35.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.28G/4.95G [01:42<00:17, 37.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.29G/4.95G [01:42<00:17, 38.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.30G/4.95G [01:43<00:16, 39.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.31G/4.95G [01:43<00:15, 40.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.32G/4.95G [01:43<00:15, 40.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.33G/4.95G [01:43<00:15, 40.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.34G/4.95G [01:44<00:13, 44.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.35G/4.95G [01:44<00:12, 45.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.36G/4.95G [01:44<00:13, 44.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.37G/4.95G [01:44<00:13, 43.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.38G/4.95G [01:45<00:12, 43.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.39G/4.95G [01:45<00:12, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.40G/4.95G [01:45<00:12, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.41G/4.95G [01:45<00:12, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.42G/4.95G [01:46<00:12, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.44G/4.95G [01:46<00:12, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.45G/4.95G [01:46<00:11, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.46G/4.95G [01:46<00:11, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.47G/4.95G [01:47<00:11, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.48G/4.95G [01:47<00:11, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.49G/4.95G [01:47<00:10, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.50G/4.95G [01:47<00:10, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.51G/4.95G [01:48<00:10, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.52G/4.95G [01:48<00:10, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.53G/4.95G [01:48<00:09, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.54G/4.95G [01:48<00:09, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.55G/4.95G [01:49<00:09, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.56G/4.95G [01:49<00:09, 41.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.57G/4.95G [01:49<00:08, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.58G/4.95G [01:49<00:08, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.59G/4.95G [01:50<00:08, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.60G/4.95G [01:50<00:08, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.61G/4.95G [01:50<00:07, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.62G/4.95G [01:50<00:07, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.63G/4.95G [01:51<00:07, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.65G/4.95G [01:51<00:07, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.66G/4.95G [01:51<00:06, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.67G/4.95G [01:51<00:06, 41.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.68G/4.95G [01:52<00:06, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.69G/4.95G [01:52<00:06, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.70G/4.95G [01:52<00:05, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.71G/4.95G [01:52<00:05, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.72G/4.95G [01:53<00:05, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.73G/4.95G [01:53<00:05, 40.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.74G/4.95G [01:53<00:05, 41.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.75G/4.95G [01:53<00:04, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.76G/4.95G [01:54<00:04, 41.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.77G/4.95G [01:54<00:03, 44.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.78G/4.95G [01:54<00:03, 45.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.79G/4.95G [01:54<00:03, 44.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.80G/4.95G [01:55<00:03, 44.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.81G/4.95G [01:55<00:03, 43.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.82G/4.95G [01:55<00:02, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.83G/4.95G [01:55<00:02, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.84G/4.95G [01:56<00:02, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.85G/4.95G [01:56<00:02, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.87G/4.95G [01:56<00:01, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.88G/4.95G [01:56<00:01, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.89G/4.95G [01:57<00:01, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.90G/4.95G [01:57<00:01, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.91G/4.95G [01:57<00:00, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.92G/4.95G [01:57<00:00, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 4.93G/4.95G [01:58<00:00, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 4.94G/4.95G [01:58<00:00, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 4.95G/4.95G [01:58<00:00, 41.7MB/s]\n",
            "Downloading shards:  50% 1/2 [01:59<01:59, 119.04s/it]\n",
            "model-00002-of-00002.safetensors:   0% 0.00/67.1M [00:00<?, ?B/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 10.5M/67.1M [00:00<00:01, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  31% 21.0M/67.1M [00:00<00:01, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  47% 31.5M/67.1M [00:00<00:00, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  62% 41.9M/67.1M [00:01<00:00, 41.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  78% 52.4M/67.1M [00:01<00:00, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  94% 62.9M/67.1M [00:01<00:00, 41.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 67.1M/67.1M [00:01<00:00, 41.6MB/s]\n",
            "Downloading shards: 100% 2/2 [02:01<00:00, 60.77s/it]\n",
            "[INFO|modeling_utils.py:1670] 2025-01-15 11:58:27,427 >> Instantiating GemmaForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:1096] 2025-01-15 11:58:27,429 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "[WARNING|logging.py:328] 2025-01-15 11:58:27,548 >> `config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
            "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
            "`config.hidden_activation` if you want to override this behaviour.\n",
            "See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
            "Loading checkpoint shards: 100% 2/2 [00:24<00:00, 12.36s/it]\n",
            "[INFO|modeling_utils.py:4800] 2025-01-15 11:58:52,421 >> All model checkpoint weights were used when initializing GemmaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4808] 2025-01-15 11:58:52,421 >> All the weights of GemmaForCausalLM were initialized from the model checkpoint at google/gemma-2b-it.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GemmaForCausalLM for predictions without further training.\n",
            "generation_config.json: 100% 137/137 [00:00<00:00, 902kB/s]\n",
            "[INFO|configuration_utils.py:1051] 2025-01-15 11:58:52,934 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/generation_config.json\n",
            "[INFO|configuration_utils.py:1096] 2025-01-15 11:58:52,935 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "[INFO|2025-01-15 11:58:53] llamafactory.model.model_utils.checkpointing:157 >> Gradient checkpointing enabled.\n",
            "[INFO|2025-01-15 11:58:53] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-01-15 11:58:53] llamafactory.model.adapter:157 >> Upcasting trainable params to float32.\n",
            "[INFO|2025-01-15 11:58:53] llamafactory.model.adapter:157 >> Fine-tuning method: LoRA\n",
            "[INFO|2025-01-15 11:58:53] llamafactory.model.model_utils.misc:157 >> Found linear modules: up_proj,o_proj,v_proj,gate_proj,down_proj,k_proj,q_proj\n",
            "[INFO|2025-01-15 11:58:53] llamafactory.model.loader:157 >> trainable params: 9,805,824 || all params: 2,515,978,240 || trainable%: 0.3897\n",
            "[INFO|trainer.py:698] 2025-01-15 11:58:53,591 >> Using auto half precision backend\n",
            "[INFO|2025-01-15 11:58:53] llamafactory.train.trainer_utils:157 >> Using LoRA+ optimizer with loraplus lr ratio 16.00.\n",
            "[INFO|trainer.py:2313] 2025-01-15 11:58:53,945 >> ***** Running training *****\n",
            "[INFO|trainer.py:2314] 2025-01-15 11:58:53,945 >>   Num examples = 500\n",
            "[INFO|trainer.py:2315] 2025-01-15 11:58:53,945 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2316] 2025-01-15 11:58:53,945 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:2319] 2025-01-15 11:58:53,945 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:2320] 2025-01-15 11:58:53,945 >>   Gradient Accumulation steps = 4\n",
            "[INFO|trainer.py:2321] 2025-01-15 11:58:53,945 >>   Total optimization steps = 186\n",
            "[INFO|trainer.py:2322] 2025-01-15 11:58:53,948 >>   Number of trainable parameters = 9,805,824\n",
            "[INFO|integration_utils.py:812] 2025-01-15 11:58:53,955 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/LLaMA-Factory/wandb/run-20250115_120213-8ggxya68\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgemma_lora\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/urisem625-Konkuk%20University/llamafactory\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/urisem625-Konkuk%20University/llamafactory/runs/8ggxya68\u001b[0m\n",
            "{'loss': 3.984, 'grad_norm': 5.212400436401367, 'learning_rate': 0.0002631578947368421, 'epoch': 0.16}\n",
            "  8% 15/186 [00:49<09:01,  3.17s/it]Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/llamafactory-cli\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/cli.py\", line 112, in main\n",
            "    run_exp()\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 92, in run_exp\n",
            "    _training_function(config={\"args\": args, \"callbacks\": callbacks})\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 66, in _training_function\n",
            "    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/sft/workflow.py\", line 101, in run_sft\n",
            "    train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2122, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2474, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3572, in training_step\n",
            "    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/sft/trainer.py\", line 100, in compute_loss\n",
            "    loss = super().compute_loss(model, inputs, return_outputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3625, in compute_loss\n",
            "    outputs = model(**inputs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 820, in forward\n",
            "    return model_forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 808, in __call__\n",
            "    return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\", line 44, in decorate_autocast\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\", line 1577, in forward\n",
            "    return self.base_model(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py\", line 188, in forward\n",
            "    return self.model.forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 170, in new_forward\n",
            "    output = module._old_forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gemma/modeling_gemma.py\", line 1091, in forward\n",
            "    loss = self.loss_function(logits, labels, self.vocab_size, **loss_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/loss/loss_utils.py\", line 46, in ForCausalLMLoss\n",
            "    loss = fixed_cross_entropy(shift_logits, shift_labels, num_items_in_batch, ignore_index, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/loss/loss_utils.py\", line 26, in fixed_cross_entropy\n",
            "    loss = nn.functional.cross_entropy(source, target, ignore_index=ignore_index, reduction=reduction)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 3479, in cross_entropy\n",
            "    return torch._C._nn.cross_entropy_loss(\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.91 GiB. GPU 0 has a total capacity of 14.75 GiB of which 1.15 GiB is free. Process 81020 has 13.59 GiB memory in use. Of the allocated memory 12.22 GiB is allocated by PyTorch, and 1.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/llamafactory-cli\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/cli.py\", line 112, in main\n",
            "    run_exp()\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 92, in run_exp\n",
            "    _training_function(config={\"args\": args, \"callbacks\": callbacks})\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 66, in _training_function\n",
            "    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/sft/workflow.py\", line 101, in run_sft\n",
            "    train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2122, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2474, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3572, in training_step\n",
            "    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/sft/trainer.py\", line 100, in compute_loss\n",
            "    loss = super().compute_loss(model, inputs, return_outputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3625, in compute_loss\n",
            "    outputs = model(**inputs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 820, in forward\n",
            "    return model_forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 808, in __call__\n",
            "    return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\", line 44, in decorate_autocast\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\", line 1577, in forward\n",
            "    return self.base_model(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py\", line 188, in forward\n",
            "    return self.model.forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 170, in new_forward\n",
            "    output = module._old_forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gemma/modeling_gemma.py\", line 1091, in forward\n",
            "    loss = self.loss_function(logits, labels, self.vocab_size, **loss_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/loss/loss_utils.py\", line 46, in ForCausalLMLoss\n",
            "    loss = fixed_cross_entropy(shift_logits, shift_labels, num_items_in_batch, ignore_index, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/loss/loss_utils.py\", line 26, in fixed_cross_entropy\n",
            "    loss = nn.functional.cross_entropy(source, target, ignore_index=ignore_index, reduction=reduction)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 3479, in cross_entropy\n",
            "    return torch._C._nn.cross_entropy_loss(\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.91 GiB. GPU 0 has a total capacity of 14.75 GiB of which 1.15 GiB is free. Process 81020 has 13.59 GiB memory in use. Of the allocated memory 12.22 GiB is allocated by PyTorch, and 1.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mgemma_lora\u001b[0m at: \u001b[34mhttps://wandb.ai/urisem625-Konkuk University/llamafactory/runs/8ggxya68\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250115_120213-8ggxya68/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "    stage=\"sft\",  # do supervised fine-tuning\n",
        "    do_train=True,\n",
        "    model_name_or_path=\"google/gemma-2b-it\",  # use bnb-4bit-quantized Gemma 2B model\n",
        "    dataset=\"train_transformed\",  # use the demo alpaca datasets\n",
        "    template=\"gemma\",  # use Gemma prompt template\n",
        "    finetuning_type=\"lora\",  # use LoRA adapters to save memory\n",
        "    lora_target=\"all\",  # attach LoRA adapters to all linear layers\n",
        "    output_dir=\"gemma_lora\",  # the path to save LoRA adapters\n",
        "    per_device_train_batch_size=2,  # the batch size\n",
        "    gradient_accumulation_steps=4,  # the gradient accumulation steps\n",
        "    lr_scheduler_type=\"cosine\",  # use cosine learning rate scheduler\n",
        "    logging_steps=10,  # log every 10 steps\n",
        "    warmup_ratio=0.1,  # use warmup scheduler\n",
        "    save_steps=1000,  # save checkpoint every 1000 steps\n",
        "    learning_rate=5e-4,  # the learning rate\n",
        "    num_train_epochs=3.0,  # the epochs of training\n",
        "    max_samples=500,  # use 500 examples in each dataset\n",
        "    max_grad_norm=1.0,  # clip gradient norm to 1.0\n",
        "    quantization_bit=4,  # use 4-bit QLoRA\n",
        "    loraplus_lr_ratio=16.0,  # use LoRA+ algorithm with lambda=16.0\n",
        "    fp16=True,  # use float16 mixed precision training\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"train_gemma.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli train train_gemma.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hJbdNtZrANr"
      },
      "source": [
        "## Run inference in a chat setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pGX3hLubhkJ",
        "outputId": "714fd212-ba42-4d8b-89d6-93b04135070d",
        "colab": {
          "referenced_widgets": [
            "3d1b6ab8ca8e4bb5978fe2b32d9f09bc"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/LLaMA-Factory/src\n",
            "/content/LLaMA-Factory\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[INFO|tokenization_utils_base.py:2108] 2024-06-02 01:59:02,909 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/2ac59a5d7bf4e1425010f0d457dde7d146658953/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-06-02 01:59:02,910 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/2ac59a5d7bf4e1425010f0d457dde7d146658953/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-06-02 01:59:02,912 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-06-02 01:59:02,914 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/2ac59a5d7bf4e1425010f0d457dde7d146658953/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-06-02 01:59:02,916 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/2ac59a5d7bf4e1425010f0d457dde7d146658953/tokenizer_config.json\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:733] 2024-06-02 01:59:04,168 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/2ac59a5d7bf4e1425010f0d457dde7d146658953/config.json\n",
            "[INFO|configuration_utils.py:796] 2024-06-02 01:59:04,173 >> Model config GemmaConfig {\n",
            "  \"_name_or_path\": \"google/gemma-2b\",\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.41.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "06/02/2024 01:59:04 - INFO - llamafactory.model.utils.quantization - Quantizing model to 4 bit.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:llamafactory.model.utils.quantization:Quantizing model to 4 bit.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "06/02/2024 01:59:04 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:llamafactory.model.patcher:Using KV cache for faster generation.\n",
            "[INFO|modeling_utils.py:3474] 2024-06-02 01:59:04,316 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/2ac59a5d7bf4e1425010f0d457dde7d146658953/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1519] 2024-06-02 01:59:04,322 >> Instantiating GemmaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:962] 2024-06-02 01:59:04,324 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "[WARNING|logging.py:329] 2024-06-02 01:59:04,333 >> `config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
            "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
            "`config.hidden_activation` if you want to override this behaviour.\n",
            "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3d1b6ab8ca8e4bb5978fe2b32d9f09bc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[INFO|modeling_utils.py:4280] 2024-06-02 01:59:10,951 >> All model checkpoint weights were used when initializing GemmaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4288] 2024-06-02 01:59:10,956 >> All the weights of GemmaForCausalLM were initialized from the model checkpoint at google/gemma-2b.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GemmaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:917] 2024-06-02 01:59:10,993 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/2ac59a5d7bf4e1425010f0d457dde7d146658953/generation_config.json\n",
            "[INFO|configuration_utils.py:962] 2024-06-02 01:59:10,995 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "06/02/2024 01:59:11 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:llamafactory.model.utils.attention:Using torch SDPA for faster training and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "06/02/2024 01:59:11 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:llamafactory.model.adapter:Upcasting trainable params to float32.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "06/02/2024 01:59:11 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:llamafactory.model.adapter:Fine-tuning method: LoRA\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "06/02/2024 01:59:11 - INFO - llamafactory.model.adapter - Loaded adapter(s): gemma_lora\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:llamafactory.model.adapter:Loaded adapter(s): gemma_lora\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "06/02/2024 01:59:11 - INFO - llamafactory.model.loader - all params: 2515978240\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:llamafactory.model.loader:all params: 2515978240\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to the CLI application, use `clear` to remove the history, use `exit` to exit the application.\n",
            "\n",
            "User: where is Chicago?\n",
            "Assistant: Chicago is located in the U.S. state of Illinois, and is the third most populous city in the United States.\n",
            "\n",
            "User: exit\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from llamafactory.chat import ChatModel\n",
        "from llamafactory.extras.misc import torch_gc\n",
        "\n",
        "# LLaMA-Factory 경로 설정\n",
        "%cd /content/LLaMA-Factory/src/\n",
        "\n",
        "# 모델 초기화\n",
        "args = dict(\n",
        "    model_name_or_path=\"google/gemma-2b-it\",  # use Gemma 2B model\n",
        "    adapter_name_or_path=\"gemma_lora\",  # load the saved LoRA adapters\n",
        "    template=\"gemma\",  # same to the one in training\n",
        "    finetuning_type=\"lora\",  # same to the one in training\n",
        "    quantization_bit=4,  # load 4-bit quantized model\n",
        ")\n",
        "chat_model = ChatModel(args)\n",
        "\n",
        "# test.csv 파일 로드\n",
        "test_file_path = \"/content/test.csv\"  # 파일 경로\n",
        "test_data = pd.read_csv(test_file_path)\n",
        "\n",
        "# 복원 결과를 저장할 리스트\n",
        "results = []\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"Restore the following phonetically encoded text to its original meaning.\"\n",
        "\n",
        "print(\"Processing inputs...\")\n",
        "\n",
        "for idx, row in test_data.iterrows():\n",
        "    input_text = row['input']\n",
        "    query = f\"{instruction}\\nInput: {input_text}\"\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": query}]\n",
        "    response = \"\"\n",
        "\n",
        "    # 모델 추론\n",
        "    for new_text in chat_model.stream_chat(messages):\n",
        "        response += new_text\n",
        "\n",
        "    # 결과 저장\n",
        "    results.append({\"ID\": row['ID'], \"output\": response.strip()})\n",
        "\n",
        "    # 메모리 관리\n",
        "    torch_gc()\n",
        "\n",
        "print(\"All inputs processed.\")\n"
      ],
      "metadata": {
        "id": "9dkTNUPB2fvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결과를 DataFrame으로 변환\n",
        "submission_df = pd.DataFrame(results)\n",
        "\n",
        "# sample_submission.csv 형식에 맞게 저장\n",
        "output_file_path = \"/content/sample_submission.csv\"\n",
        "submission_df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(f\"Results saved to {output_file_path}\")\n"
      ],
      "metadata": {
        "id": "Kyw6LRt22ghP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeZUSRbHhbV2"
      },
      "source": [
        "## Merge the LoRA adapter and upload the finetuned model to Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w84le7s5jyY_",
        "outputId": "91a5cc60-012a-4f99-9c11-347d671ff9d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/LLaMA-Factory\n",
            "2024-06-02 01:59:36.861478: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-06-02 01:59:36.861538: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-06-02 01:59:36.862938: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-06-02 01:59:38.404919: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-06-02 01:59:47,841 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/2ac59a5d7bf4e1425010f0d457dde7d146658953/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-06-02 01:59:47,842 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/2ac59a5d7bf4e1425010f0d457dde7d146658953/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-06-02 01:59:47,842 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-06-02 01:59:47,842 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/2ac59a5d7bf4e1425010f0d457dde7d146658953/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-06-02 01:59:47,842 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/2ac59a5d7bf4e1425010f0d457dde7d146658953/tokenizer_config.json\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:733] 2024-06-02 01:59:49,029 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/2ac59a5d7bf4e1425010f0d457dde7d146658953/config.json\n",
            "[INFO|configuration_utils.py:796] 2024-06-02 01:59:49,031 >> Model config GemmaConfig {\n",
            "  \"_name_or_path\": \"google/gemma-2b\",\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.41.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "06/02/2024 01:59:49 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n",
            "[INFO|modeling_utils.py:3474] 2024-06-02 01:59:49,150 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/2ac59a5d7bf4e1425010f0d457dde7d146658953/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1519] 2024-06-02 01:59:49,152 >> Instantiating GemmaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:962] 2024-06-02 01:59:49,153 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "[WARNING|logging.py:329] 2024-06-02 01:59:49,156 >> `config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
            "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
            "`config.hidden_activation` if you want to override this behaviour.\n",
            "See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
            "Loading checkpoint shards: 100% 2/2 [00:00<00:00,  2.65it/s]\n",
            "[INFO|modeling_utils.py:4280] 2024-06-02 01:59:49,958 >> All model checkpoint weights were used when initializing GemmaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4288] 2024-06-02 01:59:49,959 >> All the weights of GemmaForCausalLM were initialized from the model checkpoint at google/gemma-2b.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GemmaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:917] 2024-06-02 01:59:49,987 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/2ac59a5d7bf4e1425010f0d457dde7d146658953/generation_config.json\n",
            "[INFO|configuration_utils.py:962] 2024-06-02 01:59:49,987 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "06/02/2024 01:59:49 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.\n",
            "06/02/2024 01:59:49 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "06/02/2024 01:59:49 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
            "06/02/2024 02:00:42 - INFO - llamafactory.model.adapter - Merged 1 adapter(s).\n",
            "06/02/2024 02:00:42 - INFO - llamafactory.model.adapter - Loaded adapter(s): gemma_lora\n",
            "06/02/2024 02:00:42 - INFO - llamafactory.model.loader - all params: 2506172416\n",
            "[INFO|configuration_utils.py:472] 2024-06-02 02:00:42,108 >> Configuration saved in gemma_lora_merged/config.json\n",
            "[INFO|configuration_utils.py:731] 2024-06-02 02:00:42,108 >> Configuration saved in gemma_lora_merged/generation_config.json\n",
            "[INFO|modeling_utils.py:2626] 2024-06-02 02:01:32,681 >> The model is bigger than the maximum size per checkpoint (2GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at gemma_lora_merged/model.safetensors.index.json.\n",
            "[INFO|configuration_utils.py:472] 2024-06-02 02:01:33,029 >> Configuration saved in /tmp/tmpumli7anw/config.json\n",
            "[INFO|configuration_utils.py:731] 2024-06-02 02:01:33,030 >> Configuration saved in /tmp/tmpumli7anw/generation_config.json\n",
            "[INFO|modeling_utils.py:2626] 2024-06-02 02:06:35,330 >> The model is bigger than the maximum size per checkpoint (2GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /tmp/tmpumli7anw/model.safetensors.index.json.\n",
            "[INFO|hub.py:759] 2024-06-02 02:07:02,840 >> Uploading the following files to windmaple/gemma-2b-finetuned-model-llama-factory: generation_config.json,config.json,model-00003-of-00003.safetensors,model-00001-of-00003.safetensors,model-00002-of-00003.safetensors,README.md,model.safetensors.index.json\n",
            "model-00003-of-00003.safetensors:   0% 0.00/1.08G [00:00<?, ?B/s]\n",
            "model-00001-of-00003.safetensors:   0% 0.00/1.95G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "Upload 3 LFS files:   0% 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   0% 16.4k/1.08G [00:00<2:53:28, 104kB/s]\n",
            "model-00001-of-00003.safetensors:   0% 16.4k/1.95G [00:00<5:46:27, 93.7kB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   1% 6.72M/1.08G [00:00<00:33, 31.7MB/s] \n",
            "model-00001-of-00003.safetensors:   0% 2.87M/1.95G [00:00<02:31, 12.9MB/s]  \u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   1% 15.9M/1.08G [00:00<00:19, 56.0MB/s]\n",
            "model-00001-of-00003.safetensors:   0% 6.14M/1.95G [00:00<01:34, 20.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   0% 4.75M/1.98G [00:00<02:11, 15.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   1% 13.4M/1.95G [00:00<00:49, 39.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   1% 11.0M/1.98G [00:00<01:05, 30.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   1% 16.0M/1.98G [00:00<00:56, 34.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   1% 17.7M/1.95G [00:00<01:12, 26.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   2% 30.7M/1.98G [00:00<00:30, 64.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:   2% 22.2M/1.08G [00:00<00:45, 23.2MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   2% 26.5M/1.08G [00:01<00:43, 24.5MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   2% 43.0M/1.98G [00:01<00:37, 52.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   2% 32.0M/1.95G [00:01<01:04, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   2% 43.1M/1.95G [00:01<00:41, 45.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   3% 32.0M/1.08G [00:01<00:52, 20.2MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   4% 40.5M/1.08G [00:01<00:34, 29.8MB/s]\n",
            "model-00001-of-00003.safetensors:   3% 49.3M/1.95G [00:01<00:51, 36.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   3% 57.4M/1.95G [00:01<00:42, 44.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   4% 47.3M/1.08G [00:01<00:30, 33.9MB/s]\n",
            "model-00001-of-00003.safetensors:   3% 63.4M/1.95G [00:01<00:42, 44.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   5% 52.1M/1.08G [00:01<00:35, 29.2MB/s]\n",
            "model-00001-of-00003.safetensors:   4% 68.8M/1.95G [00:01<00:49, 37.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   6% 61.1M/1.08G [00:01<00:26, 39.1MB/s]\n",
            "model-00001-of-00003.safetensors:   4% 74.3M/1.95G [00:02<00:47, 39.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   4% 87.8M/1.98G [00:02<00:46, 40.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:   6% 66.3M/1.08G [00:02<00:31, 32.5MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   7% 74.7M/1.08G [00:02<00:24, 41.8MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   5% 99.9M/1.98G [00:02<00:46, 40.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   4% 84.5M/1.95G [00:02<00:56, 32.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   5% 104M/1.98G [00:02<00:46, 40.3MB/s] \u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   5% 91.7M/1.95G [00:02<00:48, 38.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   6% 111M/1.98G [00:02<00:43, 43.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   5% 96.1M/1.95G [00:02<00:58, 31.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   6% 115M/1.98G [00:02<00:49, 37.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   5% 107M/1.95G [00:02<00:39, 46.3MB/s] \u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   8% 89.0M/1.08G [00:03<00:34, 28.9MB/s]\n",
            "model-00001-of-00003.safetensors:   6% 113M/1.95G [00:03<00:47, 38.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   9% 95.9M/1.08G [00:03<00:28, 34.8MB/s]\n",
            "model-00001-of-00003.safetensors:   6% 120M/1.95G [00:03<00:41, 44.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   7% 134M/1.98G [00:03<00:45, 40.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   6% 125M/1.95G [00:03<00:41, 44.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  11% 116M/1.08G [00:03<00:27, 34.7MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   7% 144M/1.98G [00:03<01:15, 24.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   8% 152M/1.98G [00:03<00:56, 32.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  12% 126M/1.08G [00:03<00:21, 44.3MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   8% 157M/1.98G [00:04<00:51, 35.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   7% 134M/1.95G [00:04<01:18, 23.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   7% 140M/1.95G [00:04<01:02, 29.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  12% 132M/1.08G [00:04<00:25, 36.9MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  13% 137M/1.08G [00:04<00:24, 38.6MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   9% 173M/1.98G [00:04<00:43, 41.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  13% 144M/1.08G [00:04<00:22, 42.4MB/s]\n",
            "model-00003-of-00003.safetensors:  14% 149M/1.08G [00:04<00:24, 37.4MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  14% 155M/1.08G [00:04<00:23, 40.1MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   9% 183M/1.98G [00:04<00:48, 37.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   8% 160M/1.95G [00:04<00:56, 31.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  15% 160M/1.08G [00:04<00:25, 36.0MB/s]\n",
            "model-00003-of-00003.safetensors:  15% 167M/1.08G [00:04<00:21, 42.4MB/s]\n",
            "model-00001-of-00003.safetensors:   9% 174M/1.95G [00:04<00:41, 43.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  16% 173M/1.08G [00:05<00:20, 44.7MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  10% 200M/1.98G [00:05<00:46, 38.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  18% 190M/1.08G [00:05<00:16, 52.5MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  11% 210M/1.98G [00:05<00:56, 31.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  19% 206M/1.08G [00:05<00:14, 59.4MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  20% 213M/1.08G [00:05<00:19, 44.3MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  21% 224M/1.08G [00:06<00:19, 43.1MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  22% 233M/1.08G [00:06<00:17, 49.7MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  13% 258M/1.98G [00:06<00:36, 47.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  23% 254M/1.08G [00:06<00:13, 60.7MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  24% 262M/1.08G [00:06<00:15, 53.4MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  25% 272M/1.08G [00:07<00:14, 54.8MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  26% 281M/1.08G [00:07<00:13, 59.8MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  27% 288M/1.08G [00:07<00:16, 49.4MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  28% 299M/1.08G [00:07<00:12, 60.8MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  29% 319M/1.08G [00:07<00:11, 64.8MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  17% 336M/1.98G [00:07<00:33, 48.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  18% 350M/1.98G [00:07<00:25, 64.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  18% 358M/1.98G [00:08<00:27, 58.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  19% 368M/1.98G [00:08<00:29, 54.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  19% 383M/1.98G [00:08<00:23, 69.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  20% 391M/1.98G [00:08<00:37, 41.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  20% 400M/1.98G [00:09<00:39, 40.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  30% 326M/1.08G [00:09<00:45, 16.5MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  32% 347M/1.08G [00:09<00:26, 27.8MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  22% 432M/1.98G [00:09<00:32, 48.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  34% 367M/1.08G [00:09<00:17, 41.7MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  23% 453M/1.98G [00:10<00:33, 46.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  35% 375M/1.08G [00:10<00:19, 36.7MB/s]\n",
            "model-00001-of-00003.safetensors:   9% 183M/1.95G [00:10<06:29, 4.53MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  35% 384M/1.08G [00:10<00:19, 36.6MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  36% 392M/1.08G [00:10<00:16, 41.9MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  37% 400M/1.08G [00:10<00:14, 46.3MB/s]\n",
            "model-00001-of-00003.safetensors:  10% 192M/1.95G [00:10<04:25, 6.62MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  24% 485M/1.98G [00:10<00:35, 41.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  37% 406M/1.08G [00:10<00:17, 37.6MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  25% 490M/1.98G [00:10<00:34, 43.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  11% 208M/1.95G [00:11<02:14, 12.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  38% 411M/1.08G [00:11<00:17, 38.8MB/s]\n",
            "model-00001-of-00003.safetensors:  11% 213M/1.95G [00:11<02:00, 14.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  38% 416M/1.08G [00:11<00:19, 33.7MB/s]\n",
            "model-00001-of-00003.safetensors:  11% 220M/1.95G [00:11<01:26, 20.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  39% 422M/1.08G [00:11<00:17, 38.7MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  40% 428M/1.08G [00:11<00:16, 39.8MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  40% 432M/1.08G [00:11<00:18, 35.5MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  27% 528M/1.98G [00:11<00:25, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  41% 440M/1.08G [00:11<00:14, 43.3MB/s]\n",
            "model-00003-of-00003.safetensors:  41% 446M/1.08G [00:11<00:13, 48.0MB/s]\n",
            "model-00001-of-00003.safetensors:  12% 237M/1.95G [00:11<01:08, 24.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  27% 534M/1.98G [00:11<00:33, 43.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  42% 451M/1.08G [00:12<00:16, 38.0MB/s]\n",
            "model-00003-of-00003.safetensors:  42% 457M/1.08G [00:12<00:14, 42.3MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  27% 544M/1.98G [00:12<00:38, 37.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  43% 464M/1.08G [00:12<00:12, 47.6MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  28% 550M/1.98G [00:12<00:35, 40.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  13% 254M/1.95G [00:12<00:51, 32.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  28% 556M/1.98G [00:12<00:33, 43.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  43% 469M/1.08G [00:12<00:20, 30.3MB/s]\n",
            "model-00001-of-00003.safetensors:  14% 267M/1.95G [00:12<00:39, 42.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  44% 476M/1.08G [00:12<00:17, 35.0MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  29% 566M/1.98G [00:12<00:37, 38.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  29% 572M/1.98G [00:12<00:33, 41.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  44% 481M/1.08G [00:12<00:18, 32.8MB/s]\n",
            "model-00003-of-00003.safetensors:  45% 488M/1.08G [00:13<00:15, 39.1MB/s]\n",
            "model-00003-of-00003.safetensors:  46% 494M/1.08G [00:13<00:13, 44.5MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  29% 576M/1.98G [00:13<00:46, 30.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  29% 584M/1.98G [00:13<00:34, 40.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  46% 499M/1.08G [00:13<00:17, 33.3MB/s]\n",
            "model-00003-of-00003.safetensors:  47% 512M/1.08G [00:13<00:12, 44.9MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  30% 592M/1.98G [00:13<00:45, 30.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  31% 606M/1.98G [00:13<00:28, 48.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  16% 304M/1.95G [00:13<00:53, 30.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  48% 517M/1.08G [00:13<00:18, 30.3MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  48% 522M/1.08G [00:14<00:16, 33.5MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  31% 619M/1.98G [00:14<00:30, 44.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  49% 527M/1.08G [00:14<00:15, 36.1MB/s]\n",
            "model-00003-of-00003.safetensors:  49% 532M/1.08G [00:14<00:17, 32.2MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  50% 540M/1.08G [00:14<00:12, 42.7MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  32% 632M/1.98G [00:14<00:34, 39.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  17% 337M/1.95G [00:14<00:45, 35.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  32% 638M/1.98G [00:14<00:32, 41.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  50% 545M/1.08G [00:14<00:15, 34.5MB/s]\n",
            "model-00001-of-00003.safetensors:  18% 352M/1.95G [00:14<00:35, 44.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  51% 552M/1.08G [00:14<00:13, 38.6MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  52% 558M/1.08G [00:14<00:12, 42.3MB/s]\n",
            "model-00001-of-00003.safetensors:  18% 357M/1.95G [00:15<00:42, 37.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  33% 654M/1.98G [00:15<00:30, 44.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  52% 563M/1.08G [00:15<00:15, 33.6MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  53% 569M/1.08G [00:15<00:13, 38.4MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  34% 666M/1.98G [00:15<00:29, 44.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  53% 575M/1.08G [00:15<00:11, 43.2MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  34% 672M/1.98G [00:15<00:30, 43.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  19% 374M/1.95G [00:15<00:41, 38.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  54% 580M/1.08G [00:15<00:15, 33.1MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  34% 676M/1.98G [00:15<00:35, 37.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  54% 586M/1.08G [00:15<00:12, 38.4MB/s]\n",
            "model-00001-of-00003.safetensors:  20% 389M/1.95G [00:15<00:41, 37.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  55% 592M/1.08G [00:15<00:12, 39.5MB/s]\n",
            "model-00001-of-00003.safetensors:  20% 394M/1.95G [00:15<00:40, 38.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  55% 596M/1.08G [00:16<00:14, 32.6MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  56% 602M/1.08G [00:16<00:12, 37.1MB/s]\n",
            "model-00001-of-00003.safetensors:  21% 400M/1.95G [00:16<00:48, 31.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  36% 705M/1.98G [00:16<00:33, 38.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  56% 608M/1.08G [00:16<00:14, 32.6MB/s]\n",
            "model-00001-of-00003.safetensors:  21% 414M/1.95G [00:16<00:35, 43.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  36% 712M/1.98G [00:16<00:29, 43.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  57% 616M/1.08G [00:16<00:12, 37.6MB/s]\n",
            "model-00003-of-00003.safetensors:  57% 622M/1.08G [00:16<00:11, 41.0MB/s]\n",
            "model-00001-of-00003.safetensors:  22% 424M/1.95G [00:16<00:39, 38.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  58% 626M/1.08G [00:16<00:13, 33.3MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  37% 731M/1.98G [00:16<00:29, 42.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  58% 632M/1.08G [00:17<00:11, 38.7MB/s]\n",
            "model-00003-of-00003.safetensors:  59% 639M/1.08G [00:17<00:10, 43.8MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  37% 736M/1.98G [00:17<00:38, 32.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  23% 445M/1.95G [00:17<00:33, 45.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  60% 650M/1.08G [00:17<00:10, 40.0MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  38% 752M/1.98G [00:17<00:29, 41.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  23% 450M/1.95G [00:17<00:49, 30.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  61% 656M/1.08G [00:17<00:11, 35.6MB/s]\n",
            "model-00001-of-00003.safetensors:  23% 456M/1.95G [00:17<00:42, 34.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  39% 767M/1.98G [00:17<00:23, 52.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  62% 672M/1.08G [00:18<00:12, 33.9MB/s]\n",
            "model-00001-of-00003.safetensors:  24% 467M/1.95G [00:18<00:55, 26.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  63% 679M/1.08G [00:18<00:10, 39.2MB/s]\n",
            "model-00001-of-00003.safetensors:  24% 473M/1.95G [00:18<00:45, 32.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  63% 683M/1.08G [00:18<00:09, 41.2MB/s]\n",
            "model-00001-of-00003.safetensors:  25% 478M/1.95G [00:18<00:40, 35.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  40% 783M/1.98G [00:18<00:33, 35.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  64% 688M/1.08G [00:18<00:12, 31.4MB/s]\n",
            "model-00001-of-00003.safetensors:  25% 483M/1.95G [00:18<00:53, 27.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  64% 694M/1.08G [00:18<00:10, 36.8MB/s]\n",
            "model-00001-of-00003.safetensors:  25% 488M/1.95G [00:18<00:44, 33.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  65% 699M/1.08G [00:18<00:09, 39.2MB/s]\n",
            "model-00003-of-00003.safetensors:  65% 704M/1.08G [00:18<00:08, 42.8MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  41% 805M/1.98G [00:18<00:33, 35.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  26% 497M/1.95G [00:19<00:49, 29.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  65% 709M/1.08G [00:19<00:10, 34.3MB/s]\n",
            "model-00003-of-00003.safetensors:  66% 714M/1.08G [00:19<00:09, 37.8MB/s]\n",
            "model-00003-of-00003.safetensors:  66% 719M/1.08G [00:19<00:09, 40.0MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  41% 821M/1.98G [00:19<00:32, 35.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  26% 514M/1.95G [00:19<00:42, 33.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  67% 723M/1.08G [00:19<00:11, 31.7MB/s]\n",
            "model-00003-of-00003.safetensors:  67% 730M/1.08G [00:19<00:09, 38.3MB/s]\n",
            "model-00001-of-00003.safetensors:  27% 526M/1.95G [00:19<00:33, 42.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  68% 736M/1.08G [00:19<00:08, 40.8MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  42% 842M/1.98G [00:19<00:28, 39.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  68% 740M/1.08G [00:19<00:10, 33.9MB/s]\n",
            "model-00003-of-00003.safetensors:  69% 752M/1.08G [00:20<00:08, 37.9MB/s]\n",
            "model-00001-of-00003.safetensors:  28% 544M/1.95G [00:20<00:41, 34.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  70% 761M/1.08G [00:20<00:06, 49.8MB/s]\n",
            "model-00003-of-00003.safetensors:  71% 767M/1.08G [00:20<00:06, 51.3MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  43% 855M/1.98G [00:20<00:38, 29.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  29% 558M/1.95G [00:20<00:32, 42.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  72% 783M/1.08G [00:20<00:05, 55.2MB/s]\n",
            "model-00001-of-00003.safetensors:  29% 563M/1.95G [00:20<00:44, 31.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  44% 865M/1.98G [00:20<00:42, 26.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  29% 573M/1.95G [00:20<00:30, 44.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  44% 873M/1.98G [00:20<00:31, 34.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  73% 789M/1.08G [00:21<00:08, 34.3MB/s]\n",
            "model-00001-of-00003.safetensors:  30% 587M/1.95G [00:21<00:27, 48.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  73% 795M/1.08G [00:21<00:07, 37.6MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  45% 888M/1.98G [00:21<00:28, 38.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  74% 800M/1.08G [00:21<00:08, 33.8MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  45% 893M/1.98G [00:21<00:26, 41.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  74% 806M/1.08G [00:21<00:07, 37.9MB/s]\n",
            "model-00003-of-00003.safetensors:  75% 813M/1.08G [00:21<00:06, 44.1MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  45% 898M/1.98G [00:21<00:32, 33.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  46% 904M/1.98G [00:21<00:27, 39.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  76% 818M/1.08G [00:21<00:07, 37.2MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  46% 912M/1.98G [00:21<00:22, 47.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  76% 824M/1.08G [00:21<00:06, 40.7MB/s]\n",
            "model-00003-of-00003.safetensors:  77% 830M/1.08G [00:22<00:05, 46.7MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  46% 918M/1.98G [00:22<00:28, 37.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  47% 925M/1.98G [00:22<00:23, 44.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  32% 625M/1.95G [00:22<00:39, 33.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  33% 636M/1.95G [00:22<00:26, 49.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  47% 930M/1.98G [00:22<00:25, 40.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  78% 844M/1.08G [00:22<00:06, 34.3MB/s]\n",
            "model-00001-of-00003.safetensors:  33% 642M/1.95G [00:22<00:35, 36.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  48% 946M/1.98G [00:22<00:24, 42.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  78% 849M/1.08G [00:22<00:06, 34.1MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  48% 951M/1.98G [00:22<00:23, 44.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  34% 655M/1.95G [00:22<00:30, 42.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  79% 860M/1.08G [00:23<00:05, 37.9MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  48% 961M/1.98G [00:23<00:32, 31.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  80% 865M/1.08G [00:23<00:06, 32.3MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  80% 870M/1.08G [00:23<00:05, 37.7MB/s]\n",
            "model-00001-of-00003.safetensors:  34% 668M/1.95G [00:23<00:36, 34.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  81% 880M/1.08G [00:23<00:05, 38.9MB/s]\n",
            "model-00001-of-00003.safetensors:  35% 672M/1.95G [00:23<00:44, 28.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  49% 979M/1.98G [00:23<00:30, 33.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  82% 884M/1.08G [00:23<00:06, 31.9MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  82% 889M/1.08G [00:23<00:05, 36.0MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  50% 992M/1.98G [00:23<00:24, 41.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  83% 894M/1.08G [00:23<00:05, 34.5MB/s]\n",
            "model-00001-of-00003.safetensors:  36% 696M/1.95G [00:24<00:31, 40.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  50% 997M/1.98G [00:24<00:28, 34.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  83% 897M/1.08G [00:24<00:05, 31.3MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  83% 903M/1.08G [00:24<00:04, 36.8MB/s]\n",
            "model-00003-of-00003.safetensors:  84% 908M/1.08G [00:24<00:04, 36.6MB/s]\n",
            "model-00001-of-00003.safetensors:  37% 713M/1.95G [00:24<00:30, 40.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  84% 912M/1.08G [00:24<00:05, 30.3MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  51% 1.02G/1.98G [00:24<00:24, 39.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  85% 920M/1.08G [00:24<00:04, 40.3MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  52% 1.02G/1.98G [00:24<00:21, 45.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  86% 925M/1.08G [00:24<00:03, 42.3MB/s]\n",
            "model-00001-of-00003.safetensors:  38% 732M/1.95G [00:24<00:28, 42.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  86% 930M/1.08G [00:25<00:04, 34.5MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  87% 936M/1.08G [00:25<00:03, 40.1MB/s]\n",
            "model-00001-of-00003.safetensors:  38% 736M/1.95G [00:25<00:43, 27.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  52% 1.04G/1.98G [00:25<00:26, 34.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  53% 1.05G/1.98G [00:25<00:20, 45.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  88% 950M/1.08G [00:25<00:03, 40.4MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  88% 955M/1.08G [00:25<00:02, 42.6MB/s]\n",
            "model-00001-of-00003.safetensors:  39% 752M/1.95G [00:25<00:35, 33.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  90% 974M/1.08G [00:25<00:01, 58.1MB/s]\n",
            "model-00001-of-00003.safetensors:  39% 768M/1.95G [00:25<00:31, 37.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  53% 1.06G/1.98G [00:25<00:40, 23.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  91% 981M/1.08G [00:26<00:02, 45.6MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  91% 986M/1.08G [00:26<00:02, 46.9MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  92% 992M/1.08G [00:26<00:01, 48.6MB/s]\n",
            "model-00001-of-00003.safetensors:  40% 788M/1.95G [00:26<00:28, 40.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  92% 997M/1.08G [00:26<00:02, 39.8MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  93% 1.00G/1.08G [00:26<00:01, 44.0MB/s]\n",
            "model-00001-of-00003.safetensors:  41% 800M/1.95G [00:26<00:29, 39.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  55% 1.09G/1.98G [00:26<00:23, 38.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  93% 1.01G/1.08G [00:26<00:01, 39.7MB/s]\n",
            "model-00001-of-00003.safetensors:  42% 813M/1.95G [00:26<00:24, 46.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  94% 1.02G/1.08G [00:26<00:01, 44.6MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  95% 1.02G/1.08G [00:27<00:01, 44.9MB/s]\n",
            "model-00001-of-00003.safetensors:  42% 818M/1.95G [00:27<00:30, 37.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  56% 1.10G/1.98G [00:27<00:22, 38.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  95% 1.03G/1.08G [00:27<00:01, 39.3MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  95% 1.03G/1.08G [00:27<00:01, 40.2MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  96% 1.04G/1.08G [00:27<00:01, 43.7MB/s]\n",
            "model-00001-of-00003.safetensors:  43% 832M/1.95G [00:27<00:31, 35.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  56% 1.12G/1.98G [00:27<00:20, 42.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  96% 1.04G/1.08G [00:27<00:01, 33.1MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  97% 1.05G/1.08G [00:27<00:00, 39.9MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  57% 1.13G/1.98G [00:27<00:21, 40.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  98% 1.06G/1.08G [00:27<00:00, 37.1MB/s]\n",
            "model-00003-of-00003.safetensors:  99% 1.07G/1.08G [00:28<00:00, 56.0MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  58% 1.14G/1.98G [00:28<00:21, 38.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  44% 854M/1.95G [00:28<00:33, 32.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  58% 1.15G/1.98G [00:28<00:21, 39.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  44% 859M/1.95G [00:28<00:29, 36.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  44% 864M/1.95G [00:28<00:35, 30.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  58% 1.15G/1.98G [00:28<00:27, 30.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  45% 877M/1.95G [00:28<00:22, 47.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  59% 1.16G/1.98G [00:28<00:20, 39.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  59% 1.17G/1.98G [00:28<00:17, 45.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  45% 884M/1.95G [00:28<00:25, 42.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  59% 1.17G/1.98G [00:28<00:20, 40.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  60% 1.18G/1.98G [00:29<00:15, 50.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  99% 1.08G/1.08G [00:29<00:00, 18.2MB/s]\n",
            "model-00003-of-00003.safetensors: 100% 1.08G/1.08G [00:29<00:00, 20.5MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  60% 1.19G/1.98G [00:29<00:20, 38.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  60% 1.20G/1.98G [00:29<00:15, 51.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors: 100% 1.08G/1.08G [00:29<00:00, 36.7MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:  47% 923M/1.95G [00:29<00:19, 53.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  61% 1.21G/1.98G [00:29<00:16, 45.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  48% 929M/1.95G [00:29<00:22, 44.3MB/s]\u001b[A\n",
            "\n",
            "Upload 3 LFS files:  33% 1/3 [00:29<00:59, 29.80s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  61% 1.22G/1.98G [00:29<00:16, 46.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  48% 944M/1.95G [00:29<00:16, 62.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  62% 1.23G/1.98G [00:29<00:13, 55.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  49% 952M/1.95G [00:30<00:19, 52.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  62% 1.23G/1.98G [00:30<00:15, 47.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  63% 1.24G/1.98G [00:30<00:12, 58.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  49% 960M/1.95G [00:30<00:20, 48.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  50% 972M/1.95G [00:30<00:15, 61.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  50% 979M/1.95G [00:30<00:18, 51.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  63% 1.25G/1.98G [00:30<00:18, 38.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  51% 987M/1.95G [00:30<00:17, 56.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  63% 1.26G/1.98G [00:30<00:16, 43.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  51% 994M/1.95G [00:30<00:19, 47.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  64% 1.26G/1.98G [00:30<00:20, 35.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  52% 1.01G/1.95G [00:31<00:14, 63.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  64% 1.27G/1.98G [00:31<00:15, 45.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  52% 1.01G/1.95G [00:31<00:16, 55.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  65% 1.28G/1.98G [00:31<00:17, 40.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  65% 1.29G/1.98G [00:31<00:11, 58.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  53% 1.02G/1.95G [00:31<00:20, 45.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  53% 1.04G/1.95G [00:31<00:14, 62.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  66% 1.30G/1.98G [00:31<00:14, 48.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  54% 1.05G/1.95G [00:31<00:18, 49.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  66% 1.31G/1.98G [00:31<00:14, 45.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  67% 1.32G/1.98G [00:32<00:12, 51.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  54% 1.06G/1.95G [00:32<00:18, 48.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  55% 1.07G/1.95G [00:32<00:15, 58.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  67% 1.33G/1.98G [00:32<00:14, 44.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  68% 1.34G/1.98G [00:32<00:10, 63.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  55% 1.07G/1.95G [00:32<00:20, 43.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  56% 1.09G/1.95G [00:32<00:15, 57.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  68% 1.35G/1.98G [00:32<00:13, 47.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  56% 1.10G/1.95G [00:32<00:16, 50.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  69% 1.36G/1.98G [00:32<00:14, 42.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  69% 1.37G/1.98G [00:33<00:10, 58.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  57% 1.10G/1.95G [00:33<00:18, 44.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  57% 1.12G/1.95G [00:33<00:13, 61.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  70% 1.38G/1.98G [00:33<00:11, 52.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  70% 1.39G/1.98G [00:33<00:12, 45.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  58% 1.13G/1.95G [00:33<00:19, 42.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  71% 1.40G/1.98G [00:33<00:11, 52.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  58% 1.13G/1.95G [00:33<00:17, 46.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  71% 1.41G/1.98G [00:33<00:12, 45.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  72% 1.42G/1.98G [00:33<00:08, 64.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  59% 1.14G/1.95G [00:33<00:22, 35.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  72% 1.43G/1.98G [00:34<00:11, 49.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  59% 1.15G/1.95G [00:34<00:22, 34.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  73% 1.44G/1.98G [00:34<00:11, 46.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  60% 1.16G/1.95G [00:34<00:17, 45.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  73% 1.45G/1.98G [00:34<00:10, 50.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  60% 1.17G/1.95G [00:34<00:20, 37.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  73% 1.46G/1.98G [00:34<00:10, 48.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  61% 1.18G/1.95G [00:34<00:16, 45.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  74% 1.47G/1.98G [00:34<00:09, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  74% 1.47G/1.98G [00:34<00:08, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  61% 1.19G/1.95G [00:35<00:18, 41.4MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  62% 1.20G/1.95G [00:35<00:12, 57.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  75% 1.48G/1.98G [00:35<00:11, 45.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  62% 1.21G/1.95G [00:35<00:14, 50.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  75% 1.49G/1.98G [00:35<00:13, 36.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  62% 1.22G/1.95G [00:35<00:16, 44.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  76% 1.50G/1.98G [00:35<00:09, 50.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  63% 1.22G/1.95G [00:35<00:14, 50.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  76% 1.51G/1.98G [00:35<00:10, 44.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  63% 1.23G/1.95G [00:36<00:18, 39.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  64% 1.25G/1.95G [00:36<00:12, 57.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  77% 1.52G/1.98G [00:36<00:10, 42.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  77% 1.53G/1.98G [00:36<00:07, 58.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  64% 1.26G/1.95G [00:36<00:13, 49.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  78% 1.54G/1.98G [00:36<00:08, 50.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  65% 1.26G/1.95G [00:36<00:14, 46.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  65% 1.27G/1.95G [00:36<00:12, 52.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  78% 1.55G/1.98G [00:36<00:09, 43.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  66% 1.28G/1.95G [00:36<00:14, 47.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  79% 1.57G/1.98G [00:36<00:07, 57.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  66% 1.29G/1.95G [00:36<00:12, 51.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  79% 1.57G/1.98G [00:37<00:07, 53.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  80% 1.58G/1.98G [00:37<00:07, 50.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  67% 1.30G/1.95G [00:37<00:19, 33.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  81% 1.60G/1.98G [00:37<00:05, 67.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  67% 1.31G/1.95G [00:37<00:14, 45.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  81% 1.61G/1.98G [00:37<00:06, 58.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  67% 1.31G/1.95G [00:37<00:14, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  68% 1.32G/1.95G [00:37<00:11, 53.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  82% 1.62G/1.98G [00:37<00:07, 48.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  82% 1.63G/1.98G [00:37<00:05, 64.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  68% 1.33G/1.95G [00:38<00:13, 46.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  69% 1.34G/1.95G [00:38<00:10, 57.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  69% 1.35G/1.95G [00:38<00:12, 48.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  70% 1.36G/1.95G [00:38<00:12, 47.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  71% 1.37G/1.95G [00:38<00:08, 64.4MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  71% 1.38G/1.95G [00:38<00:10, 52.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  71% 1.39G/1.95G [00:39<00:12, 44.4MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  72% 1.41G/1.95G [00:39<00:08, 60.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  73% 1.42G/1.95G [00:39<00:09, 55.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  73% 1.42G/1.95G [00:39<00:11, 46.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  74% 1.44G/1.95G [00:39<00:08, 61.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  74% 1.45G/1.95G [00:40<00:08, 57.4MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  75% 1.46G/1.95G [00:40<00:10, 47.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  76% 1.47G/1.95G [00:40<00:07, 63.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  76% 1.48G/1.95G [00:40<00:08, 57.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  83% 1.64G/1.98G [00:40<00:32, 10.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  76% 1.49G/1.95G [00:40<00:09, 46.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  83% 1.65G/1.98G [00:41<00:24, 13.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  77% 1.50G/1.95G [00:41<00:07, 59.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  84% 1.66G/1.98G [00:41<00:18, 17.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  77% 1.51G/1.95G [00:41<00:08, 52.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  84% 1.66G/1.98G [00:41<00:16, 19.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  84% 1.67G/1.98G [00:41<00:12, 25.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  78% 1.52G/1.95G [00:41<00:08, 47.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  79% 1.54G/1.95G [00:41<00:06, 64.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  85% 1.68G/1.98G [00:41<00:11, 26.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  86% 1.69G/1.98G [00:41<00:07, 40.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  79% 1.54G/1.95G [00:41<00:07, 51.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  86% 1.70G/1.98G [00:42<00:07, 38.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  80% 1.55G/1.95G [00:42<00:08, 46.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  80% 1.57G/1.95G [00:42<00:06, 62.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  86% 1.71G/1.98G [00:42<00:07, 33.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  87% 1.72G/1.98G [00:42<00:05, 44.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  81% 1.57G/1.95G [00:42<00:07, 51.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  81% 1.58G/1.95G [00:42<00:07, 52.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  87% 1.73G/1.98G [00:42<00:06, 37.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  82% 1.59G/1.95G [00:42<00:07, 45.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  88% 1.74G/1.98G [00:42<00:05, 44.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  82% 1.59G/1.95G [00:42<00:07, 47.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  82% 1.60G/1.95G [00:43<00:08, 39.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  88% 1.75G/1.98G [00:43<00:06, 37.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  83% 1.61G/1.95G [00:43<00:07, 47.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  89% 1.75G/1.98G [00:43<00:05, 44.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  83% 1.62G/1.95G [00:43<00:06, 52.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  89% 1.76G/1.98G [00:43<00:05, 37.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  83% 1.62G/1.95G [00:43<00:08, 39.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  89% 1.77G/1.98G [00:43<00:04, 50.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  84% 1.63G/1.95G [00:43<00:07, 42.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  90% 1.78G/1.98G [00:43<00:04, 41.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  84% 1.63G/1.95G [00:43<00:08, 36.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  90% 1.79G/1.98G [00:44<00:03, 53.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  84% 1.64G/1.95G [00:44<00:07, 42.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  91% 1.80G/1.98G [00:44<00:03, 46.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  85% 1.65G/1.95G [00:44<00:07, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  85% 1.66G/1.95G [00:44<00:05, 48.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  91% 1.81G/1.98G [00:44<00:03, 45.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  92% 1.82G/1.98G [00:44<00:02, 59.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  85% 1.66G/1.95G [00:44<00:06, 43.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  86% 1.68G/1.95G [00:44<00:04, 63.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  92% 1.83G/1.98G [00:44<00:03, 48.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  93% 1.84G/1.98G [00:44<00:02, 58.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  87% 1.69G/1.95G [00:44<00:05, 51.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  87% 1.69G/1.95G [00:45<00:04, 58.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  93% 1.85G/1.98G [00:45<00:02, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  94% 1.85G/1.98G [00:45<00:02, 62.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  87% 1.70G/1.95G [00:45<00:04, 49.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  94% 1.86G/1.98G [00:45<00:02, 49.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  88% 1.71G/1.95G [00:45<00:04, 48.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  88% 1.72G/1.95G [00:45<00:03, 60.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  94% 1.87G/1.98G [00:45<00:02, 46.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  95% 1.89G/1.98G [00:45<00:01, 65.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  89% 1.73G/1.95G [00:45<00:04, 48.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  96% 1.90G/1.98G [00:45<00:01, 60.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  89% 1.74G/1.95G [00:45<00:03, 55.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  96% 1.90G/1.98G [00:46<00:01, 58.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  90% 1.75G/1.95G [00:46<00:04, 46.4MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  90% 1.76G/1.95G [00:46<00:03, 60.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  96% 1.91G/1.98G [00:46<00:01, 45.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  97% 1.92G/1.98G [00:46<00:01, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  91% 1.77G/1.95G [00:46<00:03, 52.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  97% 1.93G/1.98G [00:46<00:01, 47.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  91% 1.78G/1.95G [00:46<00:03, 45.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  92% 1.79G/1.95G [00:46<00:02, 62.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  98% 1.94G/1.98G [00:46<00:01, 42.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  98% 1.95G/1.98G [00:46<00:00, 59.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  92% 1.80G/1.95G [00:47<00:03, 45.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  99% 1.96G/1.98G [00:47<00:00, 45.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  93% 1.81G/1.95G [00:47<00:03, 40.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  94% 1.82G/1.95G [00:47<00:02, 56.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  99% 1.97G/1.98G [00:47<00:00, 37.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors: 100% 1.98G/1.98G [00:47<00:00, 51.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors: 100% 1.98G/1.98G [00:47<00:00, 41.4MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:  94% 1.84G/1.95G [00:48<00:02, 43.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  95% 1.85G/1.95G [00:48<00:01, 59.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  96% 1.86G/1.95G [00:48<00:01, 46.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  96% 1.87G/1.95G [00:48<00:01, 45.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  97% 1.89G/1.95G [00:48<00:01, 61.4MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  97% 1.89G/1.95G [00:49<00:01, 53.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  98% 1.90G/1.95G [00:49<00:00, 47.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  98% 1.92G/1.95G [00:49<00:00, 63.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  99% 1.93G/1.95G [00:49<00:00, 55.4MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors: 100% 1.95G/1.95G [00:50<00:00, 38.9MB/s]\n",
            "\n",
            "\n",
            "Upload 3 LFS files: 100% 3/3 [00:50<00:00, 16.80s/it]\n",
            "[INFO|tokenization_utils_base.py:2513] 2024-06-02 02:07:54,530 >> tokenizer config file saved in gemma_lora_merged/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2522] 2024-06-02 02:07:54,530 >> Special tokens file saved in gemma_lora_merged/special_tokens_map.json\n",
            "README.md: 100% 5.19k/5.19k [00:00<00:00, 16.0MB/s]\n",
            "[INFO|tokenization_utils_base.py:2513] 2024-06-02 02:07:55,448 >> tokenizer config file saved in /tmp/tmphfp90355/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2522] 2024-06-02 02:07:55,448 >> Special tokens file saved in /tmp/tmphfp90355/special_tokens_map.json\n",
            "[INFO|hub.py:759] 2024-06-02 02:07:56,019 >> Uploading the following files to windmaple/gemma-2b-finetuned-model-llama-factory: special_tokens_map.json,tokenizer.model,README.md,tokenizer_config.json,tokenizer.json\n",
            "tokenizer.model:   0% 0.00/4.24M [00:00<?, ?B/s]\n",
            "tokenizer.json:   0% 0.00/17.5M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "Upload 2 LFS files:   0% 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "tokenizer.model: 100% 4.24M/4.24M [00:00<00:00, 23.6MB/s]\n",
            "\n",
            "\n",
            "tokenizer.json: 100% 17.5M/17.5M [00:00<00:00, 40.0MB/s]\n",
            "\n",
            "\n",
            "Upload 2 LFS files: 100% 2/2 [00:00<00:00,  3.25it/s]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "    model_name_or_path=\"google/gemma-2b\",  # use official non-quantized Gemma 2B model\n",
        "    adapter_name_or_path=\"gemma_lora\",  # load the saved LoRA adapters\n",
        "    template=\"gemma\",  # same to the one in training\n",
        "    finetuning_type=\"lora\",  # same to the one in training\n",
        "    export_dir=\"gemma_lora_merged\",  # path to save the merged model\n",
        "    export_size=2,  # the file shard size (in GB) of the merged model\n",
        "    export_device=\"cpu\",  # the device used in export, can be chosen from `cpu` and `cuda`\n",
        "    export_hub_model_id=\"gemma-2b-finetuned-model-llama-factory\",  # your Hugging Face hub model ID\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"merge_gemma.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli export merge_gemma.json"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Finetune_with_LLaMA_Factory.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}