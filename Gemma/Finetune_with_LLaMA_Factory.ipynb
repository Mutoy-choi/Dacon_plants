{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2024 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfsDR_omdNea"
      },
      "source": [
        "# Gemma - finetune with LLaMA Factory\n",
        "\n",
        "This notebook demonstrates how to finetune Gemma with LLaMA Factory. [LLaMA Factory](https://github.com/InternLM/xtuner) is a tool that specifically designed for finetuning LLMs. LLaMA Factory wraps the Hugging Face finetuning functionality and provides a simple interface for finetuning. It's very easy to finetune Gemma with LLaMA Factory. This notebook follows very closely the official [Colab notebook](https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing) from LLaMA Factory.\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Gemma/Finetune_with_LLaMA_Factory.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwMiP7jDdAL1"
      },
      "source": [
        "## Setup\n",
        "\n",
        "### Select the Colab runtime\n",
        "To complete this tutorial, you'll need to have a Colab runtime with sufficient resources to run the Gemma model. In this case, you can use a T4 GPU:\n",
        "\n",
        "1. In the upper-right of the Colab window, select **▾ (Additional connection options)**.\n",
        "2. Select **Change runtime type**.\n",
        "3. Under **Hardware accelerator**, select **T4 GPU**.\n",
        "\n",
        "\n",
        "### Gemma setup on Hugging Face\n",
        "LLaMA Factory uses Hugging Face under the hood. So you will need to:\n",
        "\n",
        "* Get access to Gemma on [huggingface.co](huggingface.co) by accepting the Gemma license on the Hugging Face page of the specific model, i.e., [Gemma 2B](https://huggingface.co/google/gemma-2b).\n",
        "* Generate a [Hugging Face access token](https://huggingface.co/docs/hub/en/security-tokens) and configure it as a Colab secret 'HF_TOKEN'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AVvJYwne3hha"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "# Note: `userdata.get` is a Colab API. If you're not using Colab, set the env\n",
        "# vars as appropriate for your system.\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yUF4Hk5dOoz"
      },
      "source": [
        "### Install LLaMA Factory\n",
        "\n",
        "Install LLaMA Factory from source on GitHub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4pY14h6_bDrr",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a727e20-1d6f-47e9-aa01-66e7038ec46a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 20658, done.\u001b[K\n",
            "remote: Counting objects: 100% (234/234), done.\u001b[K\n",
            "remote: Compressing objects: 100% (105/105), done.\u001b[K\n",
            "remote: Total 20658 (delta 176), reused 129 (delta 129), pack-reused 20424 (from 3)\u001b[K\n",
            "Receiving objects: 100% (20658/20658), 235.49 MiB | 15.56 MiB/s, done.\n",
            "Resolving deltas: 100% (14929/14929), done.\n",
            "/content/LLaMA-Factory\n",
            "\u001b[0m\u001b[01;34massets\u001b[0m/       \u001b[01;34mdocker\u001b[0m/      LICENSE      pyproject.toml  requirements.txt  \u001b[01;34msrc\u001b[0m/\n",
            "CITATION.cff  \u001b[01;34mevaluation\u001b[0m/  Makefile     README.md       \u001b[01;34mscripts\u001b[0m/          \u001b[01;34mtests\u001b[0m/\n",
            "\u001b[01;34mdata\u001b[0m/         \u001b[01;34mexamples\u001b[0m/    MANIFEST.in  README_zh.md    setup.py\n",
            "Obtaining file:///content/LLaMA-Factory\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers<=4.46.1,>=4.41.2 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (4.46.1)\n",
            "Requirement already satisfied: datasets<=3.1.0,>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (3.1.0)\n",
            "Requirement already satisfied: accelerate<=1.0.1,>=0.34.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (1.0.1)\n",
            "Requirement already satisfied: peft<=0.12.0,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (0.12.0)\n",
            "Requirement already satisfied: trl<=0.9.6,>=0.8.6 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (0.9.6)\n",
            "Requirement already satisfied: tokenizers<0.20.4,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (0.20.3)\n",
            "Requirement already satisfied: gradio<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (4.44.1)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (1.13.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (0.8.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (0.2.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (0.8.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (4.25.5)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (0.34.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (2.10.3)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (0.115.6)\n",
            "Requirement already satisfied: sse-starlette in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (2.2.1)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (3.8.0)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (0.7.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (6.0.2)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (1.26.4)\n",
            "Requirement already satisfied: av in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (14.0.1)\n",
            "Requirement already satisfied: tyro<0.9.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (0.8.14)\n",
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (2.5.1+cu121)\n",
            "Requirement already satisfied: bitsandbytes>=0.39.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (0.45.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.0.1,>=0.34.0->llamafactory==0.9.2.dev0) (5.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.0.1,>=0.34.0->llamafactory==0.9.2.dev0) (0.27.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.0.1,>=0.34.0->llamafactory==0.9.2.dev0) (0.4.5)\n",
            "Requirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes>=0.39.0->llamafactory==0.9.2.dev0) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (3.11.10)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (4.8.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (1.3.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (0.28.1)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (6.4.5)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (2.1.5)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (3.10.12)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (10.4.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (0.9.1)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (0.15.1)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (2.2.3)\n",
            "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (12.0)\n",
            "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->llamafactory==0.9.2.dev0) (0.41.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.9.2.dev0) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.9.2.dev0) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.9.2.dev0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.9.2.dev0) (2.27.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (3.4.2)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.1->llamafactory==0.9.2.dev0) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<=4.46.1,>=4.41.2->llamafactory==0.9.2.dev0) (2024.11.6)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0->llamafactory==0.9.2.dev0) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0->llamafactory==0.9.2.dev0) (13.9.4)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0->llamafactory==0.9.2.dev0) (1.7.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.9.2.dev0) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.9.2.dev0) (0.14.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.9.2.dev0) (2.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (1.2.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (1.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (1.18.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (1.0.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (3.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.2.dev0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.2.dev0) (2.18.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (1.5.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.2.dev0) (0.1.2)\n",
            "Building wheels for collected packages: llamafactory\n",
            "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llamafactory: filename=llamafactory-0.9.2.dev0-0.editable-py3-none-any.whl size=25151 sha256=8f9b290a4e39903e6b2ab54db4d32d152715c1e970aed1582a230dafc9f35ccc\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-kffq1k2c/wheels/de/aa/c5/27b5682c5592b7c0eecc3e208f176dedf6b11a61cf2a910b85\n",
            "Successfully built llamafactory\n",
            "Installing collected packages: llamafactory\n",
            "  Attempting uninstall: llamafactory\n",
            "    Found existing installation: llamafactory 0.9.2.dev0\n",
            "    Uninstalling llamafactory-0.9.2.dev0:\n",
            "      Successfully uninstalled llamafactory-0.9.2.dev0\n",
            "Successfully installed llamafactory-0.9.2.dev0\n"
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "%rm -rf LLaMA-Factory\n",
        "!git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory\n",
        "%ls\n",
        "!pip install -e .[torch,bitsandbytes]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "UPaRFyhu4pnJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Di9D2DY5dqmw"
      },
      "source": [
        "## Finetune Gemma\n",
        "\n",
        "Kick off Gemma 2B finetuning with a [demo Alpaca dataset](https://github.com/hiyouga/LLaMA-Factory/blob/main/data/alpaca_en_demo.json). If you want to use your own dataset, follow this [guide from LLaMA Factory](https://github.com/hiyouga/LLaMA-Factory/tree/main/data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gWIzVxhwcDSw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "265f5a32-c71a-43b1-a57b-43019e691623"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "2025-01-15 12:14:35.269057: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-15 12:14:35.288315: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-15 12:14:35.294223: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-15 12:14:36.386292: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[WARNING|2025-01-15 12:14:43] llamafactory.hparams.parser:162 >> We recommend enable `upcast_layernorm` in quantized training.\n",
            "[INFO|2025-01-15 12:14:43] llamafactory.hparams.parser:380 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|configuration_utils.py:679] 2025-01-15 12:14:43,911 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\n",
            "[INFO|configuration_utils.py:746] 2025-01-15 12:14:43,912 >> Model config GemmaConfig {\n",
            "  \"_name_or_path\": \"google/gemma-2b-it\",\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-15 12:14:44,190 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-15 12:14:44,190 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-15 12:14:44,190 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-15 12:14:44,190 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-15 12:14:44,191 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:679] 2025-01-15 12:14:46,274 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\n",
            "[INFO|configuration_utils.py:746] 2025-01-15 12:14:46,276 >> Model config GemmaConfig {\n",
            "  \"_name_or_path\": \"google/gemma-2b-it\",\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-15 12:14:46,654 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-15 12:14:46,654 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-15 12:14:46,654 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-15 12:14:46,654 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-15 12:14:46,654 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer_config.json\n",
            "[INFO|2025-01-15 12:14:47] llamafactory.data.loader:157 >> Loading dataset train_transformed.json...\n",
            "Converting format of dataset: 100% 5000/5000 [00:00<00:00, 13746.96 examples/s]\n",
            "Running tokenizer on dataset: 100% 5000/5000 [00:02<00:00, 1983.12 examples/s]\n",
            "training example:\n",
            "input_ids:\n",
            "[2, 106, 1645, 108, 67556, 573, 2412, 5248, 11070, 49748, 2793, 577, 1277, 3464, 6996, 235265, 108, 239793, 35191, 85181, 238772, 235248, 239985, 247872, 249576, 235265, 235248, 242940, 235248, 242686, 238912, 243483, 240356, 235248, 246544, 235248, 235274, 240457, 236791, 235248, 252902, 241105, 241330, 235248, 453, 404, 382, 250816, 241562, 235248, 242686, 238912, 253978, 243262, 247539, 235248, 452, 384, 363, 238779, 235248, 453, 349, 384, 245334, 254670, 250036, 244604, 235248, 253982, 452, 346, 359, 452, 380, 361, 241105, 235248, 239228, 242780, 235248, 250232, 237905, 955, 137965, 452, 390, 358, 238035, 235248, 246715, 75980, 235248, 452, 356, 403, 241215, 235248, 251905, 248054, 235248, 246799, 241573, 235248, 451, 398, 380, 235265, 235248, 451, 402, 401, 453, 347, 382, 255940, 235248, 235284, 235276, 237386, 84649, 235248, 452, 357, 373, 245029, 452, 398, 385, 235248, 244315, 47250, 235248, 453, 386, 385, 245178, 235248, 451, 400, 362, 246697, 235248, 243622, 253681, 242251, 235248, 251032, 235265, 107, 108, 106, 2516, 108, 239793, 35191, 49532, 236840, 23745, 247872, 236039, 235265, 235248, 242940, 64691, 94525, 211017, 235248, 235274, 237936, 236791, 40712, 236214, 236183, 235248, 246393, 236770, 238744, 64691, 26291, 236554, 72163, 236375, 220596, 236345, 236645, 236405, 235248, 230019, 236375, 236214, 162056, 237908, 159897, 236464, 955, 23745, 237908, 241184, 93828, 84389, 171167, 23248, 240373, 235248, 246799, 236648, 226822, 235265, 235248, 242968, 241496, 236392, 235248, 235284, 235276, 237386, 84649, 32048, 239228, 238744, 226822, 47250, 31850, 236666, 28693, 238304, 38585, 254248, 240080, 226822, 235265, 107, 108]\n",
            "inputs:\n",
            "<bos><start_of_turn>user\n",
            "Restore the following phonetically encoded text to its original meaning.\n",
            "별 한 게토 았깝땀. 왜 싸람듯릭 펼 1캐를 쥰눈징 컥꺾폰 싸람믐롯섞 맒록 섧멍핥쟈닐 탯끎룐눈 녀뮤 퀼교... 야뭍툰 둠 변 닺씨 깍낄 싫훈 굣. 깸삥읊 20여 년 댜녁뵨 곧 중 쩨윌 귑푼 낙팠떤 곶.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "별 한 개도 아깝다. 왜 사람들이 별 1개를 주는지 겪어본 사람으로서 말로 설명하자니 댓글로는 너무 길고... 아무튼 두 번 다시 가길 싫은 곳. 캠핑을 20여 년 다녀본 곳 중 제일 기분 나빴던 곳.<end_of_turn>\n",
            "\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 239793, 35191, 49532, 236840, 23745, 247872, 236039, 235265, 235248, 242940, 64691, 94525, 211017, 235248, 235274, 237936, 236791, 40712, 236214, 236183, 235248, 246393, 236770, 238744, 64691, 26291, 236554, 72163, 236375, 220596, 236345, 236645, 236405, 235248, 230019, 236375, 236214, 162056, 237908, 159897, 236464, 955, 23745, 237908, 241184, 93828, 84389, 171167, 23248, 240373, 235248, 246799, 236648, 226822, 235265, 235248, 242968, 241496, 236392, 235248, 235284, 235276, 237386, 84649, 32048, 239228, 238744, 226822, 47250, 31850, 236666, 28693, 238304, 38585, 254248, 240080, 226822, 235265, 107, 108]\n",
            "labels:\n",
            "별 한 개도 아깝다. 왜 사람들이 별 1개를 주는지 겪어본 사람으로서 말로 설명하자니 댓글로는 너무 길고... 아무튼 두 번 다시 가길 싫은 곳. 캠핑을 20여 년 다녀본 곳 중 제일 기분 나빴던 곳.<end_of_turn>\n",
            "\n",
            "[INFO|configuration_utils.py:679] 2025-01-15 12:14:52,205 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\n",
            "[INFO|configuration_utils.py:746] 2025-01-15 12:14:52,206 >> Model config GemmaConfig {\n",
            "  \"_name_or_path\": \"google/gemma-2b-it\",\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|2025-01-15 12:14:52] llamafactory.model.model_utils.quantization:157 >> Quantizing model to 4 bit with bitsandbytes.\n",
            "[INFO|modeling_utils.py:3937] 2025-01-15 12:14:52,270 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1670] 2025-01-15 12:14:52,271 >> Instantiating GemmaForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:1096] 2025-01-15 12:14:52,272 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "[WARNING|logging.py:328] 2025-01-15 12:14:52,275 >> `config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
            "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
            "`config.hidden_activation` if you want to override this behaviour.\n",
            "See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
            "Loading checkpoint shards: 100% 2/2 [00:09<00:00,  4.73s/it]\n",
            "[INFO|modeling_utils.py:4800] 2025-01-15 12:15:01,849 >> All model checkpoint weights were used when initializing GemmaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4808] 2025-01-15 12:15:01,849 >> All the weights of GemmaForCausalLM were initialized from the model checkpoint at google/gemma-2b-it.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GemmaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1051] 2025-01-15 12:15:02,099 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/generation_config.json\n",
            "[INFO|configuration_utils.py:1096] 2025-01-15 12:15:02,099 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "[INFO|2025-01-15 12:15:02] llamafactory.model.model_utils.checkpointing:157 >> Gradient checkpointing enabled.\n",
            "[INFO|2025-01-15 12:15:02] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-01-15 12:15:02] llamafactory.model.adapter:157 >> Upcasting trainable params to float32.\n",
            "[INFO|2025-01-15 12:15:02] llamafactory.model.adapter:157 >> Fine-tuning method: LoRA\n",
            "[INFO|2025-01-15 12:15:02] llamafactory.model.model_utils.misc:157 >> Found linear modules: gate_proj,o_proj,q_proj,up_proj,down_proj,v_proj,k_proj\n",
            "[INFO|2025-01-15 12:15:02] llamafactory.model.loader:157 >> trainable params: 9,805,824 || all params: 2,515,978,240 || trainable%: 0.3897\n",
            "[INFO|trainer.py:698] 2025-01-15 12:15:02,556 >> Using auto half precision backend\n",
            "[INFO|2025-01-15 12:15:02] llamafactory.train.trainer_utils:157 >> Using LoRA+ optimizer with loraplus lr ratio 16.00.\n",
            "[INFO|trainer.py:2313] 2025-01-15 12:15:02,927 >> ***** Running training *****\n",
            "[INFO|trainer.py:2314] 2025-01-15 12:15:02,927 >>   Num examples = 5,000\n",
            "[INFO|trainer.py:2315] 2025-01-15 12:15:02,928 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2316] 2025-01-15 12:15:02,928 >>   Instantaneous batch size per device = 1\n",
            "[INFO|trainer.py:2319] 2025-01-15 12:15:02,928 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:2320] 2025-01-15 12:15:02,928 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2321] 2025-01-15 12:15:02,928 >>   Total optimization steps = 1,875\n",
            "[INFO|trainer.py:2322] 2025-01-15 12:15:02,930 >>   Number of trainable parameters = 9,805,824\n",
            "[INFO|integration_utils.py:812] 2025-01-15 12:15:02,937 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33murisem625\u001b[0m (\u001b[33murisem625-Konkuk University\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/LLaMA-Factory/wandb/run-20250115_121503-i8c4jj0h\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgemma_lora\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/urisem625-Konkuk%20University/llamafactory\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/urisem625-Konkuk%20University/llamafactory/runs/i8c4jj0h\u001b[0m\n",
            "{'loss': 4.4571, 'grad_norm': 3.485372543334961, 'learning_rate': 2.6595744680851064e-05, 'epoch': 0.02}\n",
            "{'loss': 3.2299, 'grad_norm': 3.1687116622924805, 'learning_rate': 5.319148936170213e-05, 'epoch': 0.03}\n",
            "{'loss': 2.5116, 'grad_norm': 3.8325395584106445, 'learning_rate': 7.97872340425532e-05, 'epoch': 0.05}\n",
            "{'loss': 2.2168, 'grad_norm': 4.667144298553467, 'learning_rate': 0.00010638297872340425, 'epoch': 0.06}\n",
            "{'loss': 1.8217, 'grad_norm': 3.7365238666534424, 'learning_rate': 0.00013297872340425532, 'epoch': 0.08}\n",
            "{'loss': 1.6073, 'grad_norm': 5.97509241104126, 'learning_rate': 0.0001595744680851064, 'epoch': 0.1}\n",
            "{'loss': 1.3957, 'grad_norm': 8.272666931152344, 'learning_rate': 0.00018617021276595746, 'epoch': 0.11}\n",
            "{'loss': 1.2568, 'grad_norm': 6.672405242919922, 'learning_rate': 0.0002127659574468085, 'epoch': 0.13}\n",
            "{'loss': 1.1837, 'grad_norm': 7.445699691772461, 'learning_rate': 0.0002393617021276596, 'epoch': 0.14}\n",
            "{'loss': 1.1271, 'grad_norm': 6.4416823387146, 'learning_rate': 0.00026595744680851064, 'epoch': 0.16}\n",
            "{'loss': 1.0542, 'grad_norm': 7.887587547302246, 'learning_rate': 0.0002925531914893617, 'epoch': 0.18}\n",
            "{'loss': 1.1682, 'grad_norm': 9.22885513305664, 'learning_rate': 0.0003191489361702128, 'epoch': 0.19}\n",
            "{'loss': 1.2113, 'grad_norm': 17.987693786621094, 'learning_rate': 0.00034308510638297873, 'epoch': 0.21}\n",
            "{'loss': 1.2353, 'grad_norm': 13.26253890991211, 'learning_rate': 0.0003696808510638298, 'epoch': 0.22}\n",
            "{'loss': 1.1221, 'grad_norm': 11.256715774536133, 'learning_rate': 0.00039627659574468084, 'epoch': 0.24}\n",
            "{'loss': 1.2742, 'grad_norm': inf, 'learning_rate': 0.0004202127659574468, 'epoch': 0.26}\n",
            "{'loss': 1.115, 'grad_norm': 14.066423416137695, 'learning_rate': 0.00044680851063829785, 'epoch': 0.27}\n",
            "{'loss': 1.3212, 'grad_norm': 15.345487594604492, 'learning_rate': 0.00047340425531914893, 'epoch': 0.29}\n",
            "{'loss': 1.3876, 'grad_norm': 17.47888946533203, 'learning_rate': 0.0005, 'epoch': 0.3}\n",
            "{'loss': 1.5249, 'grad_norm': 18.931968688964844, 'learning_rate': 0.0004999566522018553, 'epoch': 0.32}\n",
            "{'loss': 1.3707, 'grad_norm': 33.40822982788086, 'learning_rate': 0.0004998266238396737, 'epoch': 0.34}\n",
            "{'loss': 1.5642, 'grad_norm': 32.282405853271484, 'learning_rate': 0.000499609960005001, 'epoch': 0.35}\n",
            "{'loss': 1.5024, 'grad_norm': 15.812616348266602, 'learning_rate': 0.0004993067358330386, 'epoch': 0.37}\n",
            "{'loss': 1.6068, 'grad_norm': 14.121580123901367, 'learning_rate': 0.0004989170564765879, 'epoch': 0.38}\n",
            "{'loss': 1.4847, 'grad_norm': 21.855600357055664, 'learning_rate': 0.0004984410570695858, 'epoch': 0.4}\n",
            "{'loss': 1.5439, 'grad_norm': 15.49618911743164, 'learning_rate': 0.0004978789026802419, 'epoch': 0.42}\n",
            "{'loss': 1.4123, 'grad_norm': 19.39776611328125, 'learning_rate': 0.000497230788253796, 'epoch': 0.43}\n",
            "{'loss': 1.5211, 'grad_norm': 24.164913177490234, 'learning_rate': 0.0004964969385449149, 'epoch': 0.45}\n",
            "{'loss': 1.4332, 'grad_norm': 15.78292179107666, 'learning_rate': 0.0004956776080397511, 'epoch': 0.46}\n",
            "{'loss': 1.4134, 'grad_norm': 19.98443603515625, 'learning_rate': 0.0004947730808676911, 'epoch': 0.48}\n",
            "{'loss': 1.4198, 'grad_norm': 24.847095489501953, 'learning_rate': 0.0004937836707028254, 'epoch': 0.5}\n",
            "{'loss': 1.5467, 'grad_norm': 20.634395599365234, 'learning_rate': 0.0004927097206551709, 'epoch': 0.51}\n",
            "{'loss': 1.4104, 'grad_norm': 30.812091827392578, 'learning_rate': 0.0004915516031516863, 'epoch': 0.53}\n",
            "{'loss': 1.6106, 'grad_norm': 11.78512191772461, 'learning_rate': 0.000490309719807122, 'epoch': 0.54}\n",
            "{'loss': 1.1992, 'grad_norm': 15.390460968017578, 'learning_rate': 0.0004889845012847463, 'epoch': 0.56}\n",
            "{'loss': 1.3995, 'grad_norm': 22.93448829650879, 'learning_rate': 0.00048757640714699926, 'epoch': 0.58}\n",
            "{'loss': 1.3088, 'grad_norm': 17.463214874267578, 'learning_rate': 0.0004860859256961244, 'epoch': 0.59}\n",
            "{'loss': 1.4974, 'grad_norm': 18.724365234375, 'learning_rate': 0.00048451357380483427, 'epoch': 0.61}\n",
            "{'loss': 1.2388, 'grad_norm': 14.497045516967773, 'learning_rate': 0.00048285989673706826, 'epoch': 0.62}\n",
            "{'loss': 1.4854, 'grad_norm': 18.28302764892578, 'learning_rate': 0.000481125467958904, 'epoch': 0.64}\n",
            "{'loss': 1.1976, 'grad_norm': 9.63771915435791, 'learning_rate': 0.00047931088893969024, 'epoch': 0.66}\n",
            "{'loss': 1.2269, 'grad_norm': 20.012235641479492, 'learning_rate': 0.00047741678894346707, 'epoch': 0.67}\n",
            "{'loss': 1.1832, 'grad_norm': 18.803464889526367, 'learning_rate': 0.0004754438248107491, 'epoch': 0.69}\n",
            "{'loss': 1.3472, 'grad_norm': 16.972780227661133, 'learning_rate': 0.0004733926807307441, 'epoch': 0.7}\n",
            "{'loss': 1.2212, 'grad_norm': 20.74287223815918, 'learning_rate': 0.0004712640680040884, 'epoch': 0.72}\n",
            "{'loss': 1.2951, 'grad_norm': 16.496112823486328, 'learning_rate': 0.0004692826903999188, 'epoch': 0.74}\n",
            "{'loss': 1.3853, 'grad_norm': 18.134201049804688, 'learning_rate': 0.0004670089428878937, 'epoch': 0.75}\n",
            "{'loss': 1.2772, 'grad_norm': 12.589776039123535, 'learning_rate': 0.00046465994049705336, 'epoch': 0.77}\n",
            "{'loss': 1.2641, 'grad_norm': 14.172499656677246, 'learning_rate': 0.00046223649782004944, 'epoch': 0.78}\n",
            "{'loss': 1.2297, 'grad_norm': 16.582351684570312, 'learning_rate': 0.00045973945526411387, 'epoch': 0.8}\n",
            "{'loss': 1.2977, 'grad_norm': 22.23908805847168, 'learning_rate': 0.00045716967875962007, 'epoch': 0.82}\n",
            "{'loss': 1.3737, 'grad_norm': 20.70077133178711, 'learning_rate': 0.0004545280594597935, 'epoch': 0.83}\n",
            "{'loss': 1.4647, 'grad_norm': 21.23826789855957, 'learning_rate': 0.0004518155134316757, 'epoch': 0.85}\n",
            "{'loss': 1.3235, 'grad_norm': 14.405736923217773, 'learning_rate': 0.00044903298133844804, 'epoch': 0.86}\n",
            "{'loss': 1.2776, 'grad_norm': 18.230281829833984, 'learning_rate': 0.0004461814281132268, 'epoch': 0.88}\n",
            "{'loss': 1.2265, 'grad_norm': 17.9874267578125, 'learning_rate': 0.0004432618426244406, 'epoch': 0.9}\n",
            "{'loss': 1.2222, 'grad_norm': 18.766714096069336, 'learning_rate': 0.0004402752373329091, 'epoch': 0.91}\n",
            "{'loss': 1.2054, 'grad_norm': 20.64003562927246, 'learning_rate': 0.00043722264794073874, 'epoch': 0.93}\n",
            "{'loss': 1.3142, 'grad_norm': 31.967851638793945, 'learning_rate': 0.00043410513303215984, 'epoch': 0.94}\n",
            "{'loss': 1.4067, 'grad_norm': 36.809810638427734, 'learning_rate': 0.0004309237737064281, 'epoch': 0.96}\n",
            "{'loss': 1.2927, 'grad_norm': 21.977270126342773, 'learning_rate': 0.0004276796732029187, 'epoch': 0.98}\n",
            "{'loss': 1.6187, 'grad_norm': inf, 'learning_rate': 0.0004247072684094071, 'epoch': 0.99}\n",
            "{'loss': 3.2026, 'grad_norm': 639.2113037109375, 'learning_rate': 0.0004216857891698057, 'epoch': 1.01}\n",
            "{'loss': 4.008, 'grad_norm': 106.37077331542969, 'learning_rate': 0.00041827206630323945, 'epoch': 1.02}\n",
            "{'loss': 2.3403, 'grad_norm': 29.88044548034668, 'learning_rate': 0.000414799989648165, 'epoch': 1.04}\n",
            "{'loss': 2.1577, 'grad_norm': 19.832963943481445, 'learning_rate': 0.0004112707632596063, 'epoch': 1.06}\n",
            "{'loss': 1.513, 'grad_norm': 20.210237503051758, 'learning_rate': 0.00040768561101110816, 'epoch': 1.07}\n",
            "{'loss': 1.5451, 'grad_norm': 51.378292083740234, 'learning_rate': 0.0004040457761703185, 'epoch': 1.09}\n",
            "{'loss': 1.9548, 'grad_norm': 27.527597427368164, 'learning_rate': 0.0004003525209678449, 'epoch': 1.1}\n",
            "{'loss': 1.5491, 'grad_norm': 26.904394149780273, 'learning_rate': 0.00039660712615953556, 'epoch': 1.12}\n",
            "{'loss': 1.4718, 'grad_norm': 26.507625579833984, 'learning_rate': 0.0003928108905823353, 'epoch': 1.14}\n",
            "{'loss': 1.445, 'grad_norm': 33.19914627075195, 'learning_rate': 0.0003889651307038724, 'epoch': 1.15}\n",
            "{'loss': 1.623, 'grad_norm': 39.60762023925781, 'learning_rate': 0.00038507118016593026, 'epoch': 1.17}\n",
            "{'loss': 1.4945, 'grad_norm': 33.00609588623047, 'learning_rate': 0.0003811303893219639, 'epoch': 1.18}\n",
            "{'loss': 1.7012, 'grad_norm': 40.93927001953125, 'learning_rate': 0.00037714412476882184, 'epoch': 1.2}\n",
            "{'loss': 1.7412, 'grad_norm': 43.235836029052734, 'learning_rate': 0.0003731137688728335, 'epoch': 1.22}\n",
            "{'loss': 1.6485, 'grad_norm': 20.572492599487305, 'learning_rate': 0.0003690407192904297, 'epoch': 1.23}\n",
            "{'loss': 1.5333, 'grad_norm': 23.066478729248047, 'learning_rate': 0.0003649263884834594, 'epoch': 1.25}\n",
            "{'loss': 1.6163, 'grad_norm': 32.478851318359375, 'learning_rate': 0.0003607722032293731, 'epoch': 1.26}\n",
            "{'loss': 1.6639, 'grad_norm': 52.20892333984375, 'learning_rate': 0.0003565796041264418, 'epoch': 1.28}\n",
            "{'loss': 1.5933, 'grad_norm': 28.147371292114258, 'learning_rate': 0.00035277462249038414, 'epoch': 1.3}\n",
            "{'loss': 1.4944, 'grad_norm': 33.617000579833984, 'learning_rate': 0.0003485130532267001, 'epoch': 1.31}\n",
            "{'loss': 1.894, 'grad_norm': 55.71177673339844, 'learning_rate': 0.0003442173213714489, 'epoch': 1.33}\n",
            "{'loss': 1.5534, 'grad_norm': 34.930267333984375, 'learning_rate': 0.0003398889166087693, 'epoch': 1.34}\n",
            "{'loss': 1.6399, 'grad_norm': 24.712539672851562, 'learning_rate': 0.00033552933995318886, 'epoch': 1.36}\n",
            "{'loss': 1.6794, 'grad_norm': 58.86857986450195, 'learning_rate': 0.0003311401032290984, 'epoch': 1.38}\n",
            "{'loss': 1.6748, 'grad_norm': 56.818519592285156, 'learning_rate': 0.00032672272854647813, 'epoch': 1.39}\n",
            "{'loss': 1.3763, 'grad_norm': 34.38875961303711, 'learning_rate': 0.00032227874777305664, 'epoch': 1.41}\n",
            "{'loss': 1.5459, 'grad_norm': 24.53199005126953, 'learning_rate': 0.00031780970200308626, 'epoch': 1.42}\n",
            "{'loss': 1.6929, 'grad_norm': 38.73834991455078, 'learning_rate': 0.0003133171410229183, 'epoch': 1.44}\n",
            "{'loss': 1.5357, 'grad_norm': 31.65934944152832, 'learning_rate': 0.0003088026227735651, 'epoch': 1.46}\n",
            "{'loss': 1.3223, 'grad_norm': 21.63999366760254, 'learning_rate': 0.00030426771281043274, 'epoch': 1.47}\n",
            "{'loss': 1.3005, 'grad_norm': 26.78809928894043, 'learning_rate': 0.0002997139837604151, 'epoch': 1.49}\n",
            "{'loss': 1.2044, 'grad_norm': 22.21438980102539, 'learning_rate': 0.00029514301477653323, 'epoch': 1.5}\n",
            "{'loss': 1.3491, 'grad_norm': 20.91577911376953, 'learning_rate': 0.00029055639099031383, 'epoch': 1.52}\n",
            "{'loss': 1.1533, 'grad_norm': 21.61772918701172, 'learning_rate': 0.00028595570296209355, 'epoch': 1.54}\n",
            "{'loss': 1.324, 'grad_norm': 27.46755027770996, 'learning_rate': 0.00028134254612944, 'epoch': 1.55}\n",
            "{'loss': 1.1864, 'grad_norm': 39.30496597290039, 'learning_rate': 0.0002767185202538827, 'epoch': 1.57}\n",
            "{'loss': 1.5838, 'grad_norm': 235.6970977783203, 'learning_rate': 0.00027208522886614385, 'epoch': 1.58}\n",
            "{'loss': 1.5129, 'grad_norm': 34.48881912231445, 'learning_rate': 0.000267444278710062, 'epoch': 1.6}\n",
            " 53% 1000/1875 [55:44<54:22,  3.73s/it][INFO|trainer.py:3801] 2025-01-15 13:10:49,264 >> Saving model checkpoint to gemma_lora/checkpoint-1000\n",
            "[INFO|configuration_utils.py:679] 2025-01-15 13:10:49,884 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\n",
            "[INFO|configuration_utils.py:746] 2025-01-15 13:10:49,886 >> Model config GemmaConfig {\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2646] 2025-01-15 13:10:49,993 >> tokenizer config file saved in gemma_lora/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2655] 2025-01-15 13:10:49,994 >> Special tokens file saved in gemma_lora/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 1.2553, 'grad_norm': 20.283649444580078, 'learning_rate': 0.00026279727918540185, 'epoch': 1.62}\n",
            "{'loss': 1.1849, 'grad_norm': 16.324621200561523, 'learning_rate': 0.0002581458417897422, 'epoch': 1.63}\n",
            "{'loss': 1.0858, 'grad_norm': 17.66928482055664, 'learning_rate': 0.0002534915795596375, 'epoch': 1.65}\n",
            "{'loss': 1.0662, 'grad_norm': 26.11214256286621, 'learning_rate': 0.00024883610651124514, 'epoch': 1.66}\n",
            "{'loss': 1.1154, 'grad_norm': 15.318989753723145, 'learning_rate': 0.00024418103708061294, 'epoch': 1.68}\n",
            "{'loss': 1.1888, 'grad_norm': 24.76112174987793, 'learning_rate': 0.0002395279855638211, 'epoch': 1.7}\n",
            "{'loss': 1.2744, 'grad_norm': 18.41683578491211, 'learning_rate': 0.00023487856555717276, 'epoch': 1.71}\n",
            "{'loss': 1.0871, 'grad_norm': 14.628704071044922, 'learning_rate': 0.0002302343893976276, 'epoch': 1.73}\n",
            "{'loss': 1.1006, 'grad_norm': 19.915170669555664, 'learning_rate': 0.0002255970676036712, 'epoch': 1.74}\n",
            "{'loss': 1.0927, 'grad_norm': 23.1671142578125, 'learning_rate': 0.00022096820831681607, 'epoch': 1.76}\n",
            "{'loss': 1.0044, 'grad_norm': 26.26177978515625, 'learning_rate': 0.00021634941674392627, 'epoch': 1.78}\n",
            "{'loss': 1.0788, 'grad_norm': 14.582903861999512, 'learning_rate': 0.0002117422946005599, 'epoch': 1.79}\n",
            "{'loss': 1.0018, 'grad_norm': 22.349040985107422, 'learning_rate': 0.0002071484395555227, 'epoch': 1.81}\n",
            "{'loss': 1.247, 'grad_norm': 20.57914924621582, 'learning_rate': 0.00020256944467682416, 'epoch': 1.82}\n",
            "{'loss': 1.137, 'grad_norm': 94.98563385009766, 'learning_rate': 0.00019800689787923, 'epoch': 1.84}\n",
            "{'loss': 0.995, 'grad_norm': 47.02428436279297, 'learning_rate': 0.00019346238137360106, 'epoch': 1.86}\n",
            "{'loss': 1.0983, 'grad_norm': 19.031930923461914, 'learning_rate': 0.00018893747111821047, 'epoch': 1.87}\n",
            "{'loss': 0.9473, 'grad_norm': 17.886592864990234, 'learning_rate': 0.00018443373627222937, 'epoch': 1.89}\n",
            "{'loss': 0.8832, 'grad_norm': 22.96337890625, 'learning_rate': 0.0001799527386515697, 'epoch': 1.9}\n",
            "{'loss': 0.9106, 'grad_norm': 10.647346496582031, 'learning_rate': 0.00017549603218727425, 'epoch': 1.92}\n",
            "{'loss': 0.8606, 'grad_norm': 17.60441017150879, 'learning_rate': 0.00017106516238664062, 'epoch': 1.94}\n",
            "{'loss': 0.8718, 'grad_norm': 13.19240951538086, 'learning_rate': 0.00016666166579726665, 'epoch': 1.95}\n",
            "{'loss': 0.8095, 'grad_norm': 20.617231369018555, 'learning_rate': 0.00016228706947420257, 'epoch': 1.97}\n",
            "{'loss': 0.7652, 'grad_norm': 7.295650005340576, 'learning_rate': 0.0001579428904503955, 'epoch': 1.98}\n",
            "{'loss': 0.8098, 'grad_norm': 15.116997718811035, 'learning_rate': 0.00015363063521060884, 'epoch': 2.0}\n",
            "{'loss': 0.7553, 'grad_norm': 29.236791610717773, 'learning_rate': 0.00014935179916899995, 'epoch': 2.02}\n",
            "{'loss': 0.7867, 'grad_norm': 17.679710388183594, 'learning_rate': 0.00014510786615053718, 'epoch': 2.03}\n",
            "{'loss': 0.7671, 'grad_norm': 11.64137077331543, 'learning_rate': 0.00014090030787643497, 'epoch': 2.05}\n",
            "{'loss': 0.7968, 'grad_norm': 17.61912727355957, 'learning_rate': 0.00013673058345378748, 'epoch': 2.06}\n",
            "{'loss': 0.9618, 'grad_norm': 37.4300537109375, 'learning_rate': 0.00013260013886957537, 'epoch': 2.08}\n",
            "{'loss': 0.7693, 'grad_norm': 9.345428466796875, 'learning_rate': 0.0001285104064892233, 'epoch': 2.1}\n",
            "{'loss': 0.694, 'grad_norm': 15.09238052368164, 'learning_rate': 0.0001244628045598808, 'epoch': 2.11}\n",
            "{'loss': 0.9356, 'grad_norm': 22.047958374023438, 'learning_rate': 0.00012045873671859913, 'epoch': 2.13}\n",
            "{'loss': 0.5516, 'grad_norm': 13.889432907104492, 'learning_rate': 0.0001164995915055746, 'epoch': 2.14}\n",
            "{'loss': 0.6434, 'grad_norm': 10.804052352905273, 'learning_rate': 0.00011258674188262735, 'epoch': 2.16}\n",
            "{'loss': 0.5896, 'grad_norm': 24.27869415283203, 'learning_rate': 0.0001087215447570824, 'epoch': 2.18}\n",
            "{'loss': 0.7247, 'grad_norm': 60.769107818603516, 'learning_rate': 0.00010490534051121808, 'epoch': 2.19}\n",
            "{'loss': 0.6093, 'grad_norm': 30.44293785095215, 'learning_rate': 0.00010113945253744497, 'epoch': 2.21}\n",
            "{'loss': 0.5767, 'grad_norm': 50.18928527832031, 'learning_rate': 9.742518677937685e-05, 'epoch': 2.22}\n",
            "{'loss': 0.6495, 'grad_norm': 8.820608139038086, 'learning_rate': 9.37638312789525e-05, 'epoch': 2.24}\n",
            "{'loss': 0.6332, 'grad_norm': 7.109243869781494, 'learning_rate': 9.015665572976523e-05, 'epoch': 2.26}\n",
            "{'loss': 0.6272, 'grad_norm': 9.602099418640137, 'learning_rate': 8.660491103675564e-05, 'epoch': 2.27}\n",
            "{'loss': 0.6275, 'grad_norm': 10.75406265258789, 'learning_rate': 8.310982888241989e-05, 'epoch': 2.29}\n",
            "{'loss': 0.6384, 'grad_norm': 7.581939697265625, 'learning_rate': 7.967262129968378e-05, 'epoch': 2.3}\n",
            "{'loss': 0.6288, 'grad_norm': 17.630144119262695, 'learning_rate': 7.629448025159108e-05, 'epoch': 2.32}\n",
            "{'loss': 0.6126, 'grad_norm': 7.789453983306885, 'learning_rate': 7.297657721795189e-05, 'epoch': 2.34}\n",
            "{'loss': 0.5371, 'grad_norm': 12.657593727111816, 'learning_rate': 6.972006278909396e-05, 'epoch': 2.35}\n",
            "{'loss': 0.5377, 'grad_norm': 14.484945297241211, 'learning_rate': 6.65260662668582e-05, 'epoch': 2.37}\n",
            "{'loss': 0.5299, 'grad_norm': 19.192190170288086, 'learning_rate': 6.339569527297667e-05, 'epoch': 2.38}\n",
            "{'loss': 0.5941, 'grad_norm': 9.425503730773926, 'learning_rate': 6.033003536496928e-05, 'epoch': 2.4}\n",
            "{'loss': 0.4644, 'grad_norm': 8.159449577331543, 'learning_rate': 5.733014965969091e-05, 'epoch': 2.42}\n",
            "{'loss': 0.4711, 'grad_norm': 9.804037094116211, 'learning_rate': 5.439707846466163e-05, 'epoch': 2.43}\n",
            "{'loss': 0.5282, 'grad_norm': 12.413331031799316, 'learning_rate': 5.153183891730628e-05, 'epoch': 2.45}\n",
            "{'loss': 0.511, 'grad_norm': 11.352198600769043, 'learning_rate': 4.8735424632229186e-05, 'epoch': 2.46}\n",
            "{'loss': 0.4597, 'grad_norm': 8.947516441345215, 'learning_rate': 4.600880535664598e-05, 'epoch': 2.48}\n",
            "{'loss': 0.4997, 'grad_norm': 12.300064086914062, 'learning_rate': 4.335292663409246e-05, 'epoch': 2.5}\n",
            "{'loss': 0.5186, 'grad_norm': 9.992009162902832, 'learning_rate': 4.076870947652675e-05, 'epoch': 2.51}\n",
            "{'loss': 0.4566, 'grad_norm': 13.61175537109375, 'learning_rate': 3.8257050044938486e-05, 'epoch': 2.53}\n",
            "{'loss': 0.4626, 'grad_norm': 11.365644454956055, 'learning_rate': 3.5818819338576086e-05, 'epoch': 2.54}\n",
            "{'loss': 0.4318, 'grad_norm': 5.169131755828857, 'learning_rate': 3.345486289289942e-05, 'epoch': 2.56}\n",
            "{'loss': 0.4497, 'grad_norm': 8.929669380187988, 'learning_rate': 3.116600048636323e-05, 'epoch': 2.58}\n",
            "{'loss': 0.4809, 'grad_norm': 10.946147918701172, 'learning_rate': 2.8953025856132104e-05, 'epoch': 2.59}\n",
            "{'loss': 0.5337, 'grad_norm': 6.707210063934326, 'learning_rate': 2.6816706422826575e-05, 'epoch': 2.61}\n",
            "{'loss': 0.3809, 'grad_norm': 8.26931381225586, 'learning_rate': 2.4757783024395242e-05, 'epoch': 2.62}\n",
            "{'loss': 0.4654, 'grad_norm': 8.450024604797363, 'learning_rate': 2.2776969659205005e-05, 'epoch': 2.64}\n",
            "{'loss': 0.4276, 'grad_norm': 6.671239852905273, 'learning_rate': 2.0874953238439243e-05, 'epoch': 2.66}\n",
            "{'loss': 0.4596, 'grad_norm': 7.102659702301025, 'learning_rate': 1.905239334788897e-05, 'epoch': 2.67}\n",
            "{'loss': 0.4177, 'grad_norm': 55.86902618408203, 'learning_rate': 1.7309922019220077e-05, 'epoch': 2.69}\n",
            "{'loss': 0.4427, 'grad_norm': 8.299328804016113, 'learning_rate': 1.564814351079602e-05, 'epoch': 2.7}\n",
            "{'loss': 0.3766, 'grad_norm': 12.664266586303711, 'learning_rate': 1.4067634098131527e-05, 'epoch': 2.72}\n",
            "{'loss': 0.3801, 'grad_norm': 23.80389404296875, 'learning_rate': 1.2568941874050555e-05, 'epoch': 2.74}\n",
            "{'loss': 0.3966, 'grad_norm': 3.6905319690704346, 'learning_rate': 1.1152586558617116e-05, 'epoch': 2.75}\n",
            "{'loss': 0.3902, 'grad_norm': 7.7695136070251465, 'learning_rate': 9.819059318905782e-06, 'epoch': 2.77}\n",
            "{'loss': 0.4079, 'grad_norm': 19.33184051513672, 'learning_rate': 8.568822598673365e-06, 'epoch': 2.78}\n",
            "{'loss': 0.5371, 'grad_norm': 12.913355827331543, 'learning_rate': 7.402309957991765e-06, 'epoch': 2.8}\n",
            "{'loss': 0.3511, 'grad_norm': 3.535534381866455, 'learning_rate': 6.319925922896791e-06, 'epoch': 2.82}\n",
            "{'loss': 0.5028, 'grad_norm': 6.312447547912598, 'learning_rate': 5.322045845105817e-06, 'epoch': 2.83}\n",
            "{'loss': 0.3723, 'grad_norm': 4.611529350280762, 'learning_rate': 4.40901577185232e-06, 'epoch': 2.85}\n",
            "{'loss': 0.5225, 'grad_norm': 5.394161701202393, 'learning_rate': 3.581152325882825e-06, 'epoch': 2.86}\n",
            "{'loss': 0.3757, 'grad_norm': 8.669808387756348, 'learning_rate': 2.8387425956576953e-06, 'epoch': 2.88}\n",
            "{'loss': 0.3992, 'grad_norm': 5.525424957275391, 'learning_rate': 2.1820440357939454e-06, 'epoch': 2.9}\n",
            "{'loss': 0.3958, 'grad_norm': 8.792941093444824, 'learning_rate': 1.6112843777845443e-06, 'epoch': 2.91}\n",
            "{'loss': 0.435, 'grad_norm': 11.500012397766113, 'learning_rate': 1.1266615510249768e-06, 'epoch': 2.93}\n",
            "{'loss': 0.3964, 'grad_norm': 4.459606647491455, 'learning_rate': 7.283436141750366e-07, 'epoch': 2.94}\n",
            "{'loss': 0.4155, 'grad_norm': 7.306260585784912, 'learning_rate': 4.1646869687891887e-07, 'epoch': 2.96}\n",
            "{'loss': 0.3494, 'grad_norm': 10.160484313964844, 'learning_rate': 1.9114495186431379e-07, 'epoch': 2.98}\n",
            "{'loss': 0.369, 'grad_norm': 8.10962963104248, 'learning_rate': 5.245051743696383e-08, 'epoch': 2.99}\n",
            "100% 1875/1875 [1:43:55<00:00,  3.45s/it][INFO|trainer.py:3801] 2025-01-15 13:58:59,615 >> Saving model checkpoint to gemma_lora/checkpoint-1875\n",
            "[INFO|configuration_utils.py:679] 2025-01-15 13:59:00,293 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\n",
            "[INFO|configuration_utils.py:746] 2025-01-15 13:59:00,294 >> Model config GemmaConfig {\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2646] 2025-01-15 13:59:00,409 >> tokenizer config file saved in gemma_lora/checkpoint-1875/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2655] 2025-01-15 13:59:00,410 >> Special tokens file saved in gemma_lora/checkpoint-1875/special_tokens_map.json\n",
            "[INFO|trainer.py:2584] 2025-01-15 13:59:01,157 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 6238.2272, 'train_samples_per_second': 2.405, 'train_steps_per_second': 0.301, 'train_loss': 1.1400090069452922, 'epoch': 3.0}\n",
            "100% 1875/1875 [1:43:56<00:00,  3.33s/it]\n",
            "[INFO|trainer.py:3801] 2025-01-15 13:59:01,162 >> Saving model checkpoint to gemma_lora\n",
            "[INFO|configuration_utils.py:679] 2025-01-15 13:59:01,788 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\n",
            "[INFO|configuration_utils.py:746] 2025-01-15 13:59:01,790 >> Model config GemmaConfig {\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2646] 2025-01-15 13:59:01,908 >> tokenizer config file saved in gemma_lora/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2655] 2025-01-15 13:59:01,908 >> Special tokens file saved in gemma_lora/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  total_flos               = 35201962GF\n",
            "  train_loss               =       1.14\n",
            "  train_runtime            = 1:43:58.22\n",
            "  train_samples_per_second =      2.405\n",
            "  train_steps_per_second   =      0.301\n",
            "[INFO|modelcard.py:449] 2025-01-15 13:59:02,496 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mgemma_lora\u001b[0m at: \u001b[34mhttps://wandb.ai/urisem625-Konkuk University/llamafactory/runs/i8c4jj0h\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250115_121503-i8c4jj0h/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "    stage=\"sft\",  # do supervised fine-tuning\n",
        "    do_train=True,\n",
        "    model_name_or_path=\"google/gemma-2b-it\",  # use bnb-4bit-quantized Gemma 2B model\n",
        "    dataset=\"train_transformed\",  # use the demo alpaca datasets\n",
        "    template=\"gemma\",  # use Gemma prompt template\n",
        "    finetuning_type=\"lora\",  # use LoRA adapters to save memory\n",
        "    lora_target=\"all\",  # attach LoRA adapters to all linear layers\n",
        "    output_dir=\"gemma_lora\",  # the path to save LoRA adapters\n",
        "    per_device_train_batch_size=1,  # the batch size\n",
        "    gradient_accumulation_steps=8,  # the gradient accumulation steps\n",
        "    lr_scheduler_type=\"cosine\",  # use cosine learning rate scheduler\n",
        "    logging_steps=10,  # log every 10 steps\n",
        "    warmup_ratio=0.1,  # use warmup scheduler\n",
        "    save_steps=1000,  # save checkpoint every 1000 steps\n",
        "    learning_rate=5e-4,  # the learning rate\n",
        "    num_train_epochs=3.0,  # the epochs of training\n",
        "    max_samples=5000,  # use 500 examples in each dataset\n",
        "    max_grad_norm=1.0,  # clip gradient norm to 1.0\n",
        "    quantization_bit=4,  # use 4-bit QLoRA\n",
        "    loraplus_lr_ratio=16.0,  # use LoRA+ algorithm with lambda=16.0\n",
        "    fp16=True,  # use float16 mixed precision training\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"train_gemma.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli train train_gemma.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CBX0_WKzLbtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hJbdNtZrANr"
      },
      "source": [
        "## Run inference in a chat setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2pGX3hLubhkJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "55ce251259d1469ba547ec8d662028af",
            "40abfe1f857f4b09b7283db8f51936c3",
            "9f72082ca58d40008a34fa905339ff32",
            "751559bac92a493bbd199ac950f4b0d7",
            "2e0506749318480a8ba784c4135a7432",
            "27d5b3e787774fed95d45ffa56adc1cb",
            "b377c06e79b240b5bd94b7257624786f",
            "30600970fd644933adb3759cd67c139d",
            "80b133558ba647c890693b75839b3a18",
            "6b51dff014024503bd07a6b5a45935ff",
            "128f5c72dce2480eb73f525cfac98839"
          ]
        },
        "outputId": "c14cf340-2389-4b4e-bc20-8b8bf3ea1359"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory/src\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO|configuration_utils.py:679] 2025-01-15 14:11:38,089 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\n",
            "[INFO|configuration_utils.py:746] 2025-01-15 14:11:38,092 >> Model config GemmaConfig {\n",
            "  \"_name_or_path\": \"google/gemma-2b-it\",\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-15 14:11:38,400 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-15 14:11:38,401 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-15 14:11:38,403 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-15 14:11:38,405 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-15 14:11:38,409 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:679] 2025-01-15 14:11:40,586 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\n",
            "[INFO|configuration_utils.py:746] 2025-01-15 14:11:40,589 >> Model config GemmaConfig {\n",
            "  \"_name_or_path\": \"google/gemma-2b-it\",\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-15 14:11:40,836 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-15 14:11:40,837 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-15 14:11:40,838 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-15 14:11:40,839 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-15 14:11:40,840 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:679] 2025-01-15 14:11:42,316 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\n",
            "[INFO|configuration_utils.py:746] 2025-01-15 14:11:42,318 >> Model config GemmaConfig {\n",
            "  \"_name_or_path\": \"google/gemma-2b-it\",\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO|2025-01-15 14:11:42] llamafactory.model.model_utils.quantization:157 >> Quantizing model to 4 bit with bitsandbytes.\n",
            "[INFO|2025-01-15 14:11:42] llamafactory.model.patcher:157 >> Using KV cache for faster generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO|modeling_utils.py:3937] 2025-01-15 14:11:42,329 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1670] 2025-01-15 14:11:42,332 >> Instantiating GemmaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1096] 2025-01-15 14:11:42,335 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55ce251259d1469ba547ec8d662028af"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO|modeling_utils.py:4800] 2025-01-15 14:11:45,221 >> All model checkpoint weights were used when initializing GemmaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4808] 2025-01-15 14:11:45,224 >> All the weights of GemmaForCausalLM were initialized from the model checkpoint at google/gemma-2b-it.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GemmaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1051] 2025-01-15 14:11:45,678 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/generation_config.json\n",
            "[INFO|configuration_utils.py:1096] 2025-01-15 14:11:45,680 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO|2025-01-15 14:11:45] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-01-15 14:11:46] llamafactory.model.adapter:157 >> Loaded adapter(s): /content/LLaMA-Factory/gemma_lora/checkpoint-1875\n",
            "[INFO|2025-01-15 14:11:46] llamafactory.model.loader:157 >> all params: 2,515,978,240\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from llamafactory.chat import ChatModel\n",
        "from llamafactory.extras.misc import torch_gc\n",
        "\n",
        "# LLaMA-Factory 경로 설정\n",
        "%cd /content/LLaMA-Factory/src/\n",
        "\n",
        "# 모델 초기화\n",
        "args = dict(\n",
        "    model_name_or_path=\"google/gemma-2b-it\",  # use Gemma 2B model\n",
        "    adapter_name_or_path=\"/content/LLaMA-Factory/gemma_lora/checkpoint-1875\",  # load the saved LoRA adapters\n",
        "    template=\"gemma\",  # same to the one in training\n",
        "    finetuning_type=\"lora\",  # same to the one in training\n",
        "    quantization_bit=4,  # load 4-bit quantized model\n",
        ")\n",
        "chat_model = ChatModel(args)\n",
        "\n",
        "# test.csv 파일 로드\n",
        "test_file_path = \"/content/test.csv\"  # 파일 경로\n",
        "test_data = pd.read_csv(test_file_path)\n",
        "\n",
        "# 복원 결과를 저장할 리스트\n",
        "results = []\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "instruction = \"Restore the following phonetically encoded text to its original meaning.\"\n",
        "print(\"Processing inputs...\")\n",
        "\n",
        "# tqdm와 함께 itertuples() 사용으로 성능 최적화\n",
        "for row in tqdm(test_data.itertuples(index=False), total=len(test_data), mininterval=0.01):\n",
        "    input_text = row.input  # itertuples()는 속성 접근 방식을 사용\n",
        "    query = f\"{instruction}\\nInput: {input_text}\"\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": query}]\n",
        "    response = \"\"\n",
        "\n",
        "    # 모델 추론\n",
        "    for new_text in chat_model.stream_chat(messages):\n",
        "        response += new_text\n",
        "        print(response)\n",
        "\n",
        "    # 결과 저장\n",
        "    results.append({\"ID\": row.ID, \"output\": response.strip()})  # itertuples에서는 ID도 속성 접근\n",
        "\n",
        "    # 메모리 관리\n",
        "    torch_gc()\n",
        "\n",
        "print(\"All inputs processed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "9dkTNUPB2fvD",
        "outputId": "8a06eedd-d129-41a6-dc69-e4e5336b7b85"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing inputs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1689 [23:31<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-6f6bb3a2fdd7>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# 모델 추론\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mnew_text\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchat_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream_chat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnew_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/LLaMA-Factory/src/llamafactory/chat/chat_model.py\u001b[0m in \u001b[0;36mstream_chat\u001b[0;34m(self, messages, system, tools, images, videos, **input_kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_coroutine_threadsafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__anext__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopAsyncIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    451\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-6hME7JZV0Tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YEegAFTkW36Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결과를 DataFrame으로 변환\n",
        "submission_df = pd.DataFrame(results)\n",
        "\n",
        "# sample_submission.csv 형식에 맞게 저장\n",
        "output_file_path = \"/content/sample_submission.csv\"\n",
        "submission_df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(f\"Results saved to {output_file_path}\")\n"
      ],
      "metadata": {
        "id": "Kyw6LRt22ghP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeZUSRbHhbV2"
      },
      "source": [
        "## Merge the LoRA adapter and upload the finetuned model to Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "w84le7s5jyY_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2f91ca9-26ed-44d0-e283-925492c4c095"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "2025-01-15 13:59:11.526757: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-15 13:59:11.560856: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-15 13:59:11.571109: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-15 13:59:13.152608: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[INFO|configuration_utils.py:679] 2025-01-15 13:59:19,010 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\n",
            "[INFO|configuration_utils.py:746] 2025-01-15 13:59:19,011 >> Model config GemmaConfig {\n",
            "  \"_name_or_path\": \"google/gemma-2b-it\",\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-15 13:59:19,275 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-15 13:59:19,275 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-15 13:59:19,275 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-15 13:59:19,275 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-15 13:59:19,275 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:679] 2025-01-15 13:59:21,356 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\n",
            "[INFO|configuration_utils.py:746] 2025-01-15 13:59:21,357 >> Model config GemmaConfig {\n",
            "  \"_name_or_path\": \"google/gemma-2b-it\",\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-15 13:59:21,625 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-15 13:59:21,625 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-15 13:59:21,625 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-15 13:59:21,625 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-15 13:59:21,625 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:679] 2025-01-15 13:59:23,036 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\n",
            "[INFO|configuration_utils.py:746] 2025-01-15 13:59:23,037 >> Model config GemmaConfig {\n",
            "  \"_name_or_path\": \"google/gemma-2b-it\",\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|2025-01-15 13:59:23] llamafactory.model.patcher:157 >> Using KV cache for faster generation.\n",
            "[INFO|modeling_utils.py:3937] 2025-01-15 13:59:23,077 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1670] 2025-01-15 13:59:23,078 >> Instantiating GemmaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1096] 2025-01-15 13:59:23,079 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "[WARNING|logging.py:328] 2025-01-15 13:59:23,081 >> `config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
            "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
            "`config.hidden_activation` if you want to override this behaviour.\n",
            "See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
            "Loading checkpoint shards: 100% 2/2 [00:00<00:00,  2.95it/s]\n",
            "[INFO|modeling_utils.py:4800] 2025-01-15 13:59:23,794 >> All model checkpoint weights were used when initializing GemmaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4808] 2025-01-15 13:59:23,794 >> All the weights of GemmaForCausalLM were initialized from the model checkpoint at google/gemma-2b-it.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GemmaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1051] 2025-01-15 13:59:24,037 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/generation_config.json\n",
            "[INFO|configuration_utils.py:1096] 2025-01-15 13:59:24,037 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "[INFO|2025-01-15 13:59:24] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-01-15 13:59:52] llamafactory.model.adapter:157 >> Merged 1 adapter(s).\n",
            "[INFO|2025-01-15 13:59:52] llamafactory.model.adapter:157 >> Loaded adapter(s): gemma_lora\n",
            "[INFO|2025-01-15 13:59:52] llamafactory.model.loader:157 >> all params: 2,506,172,416\n",
            "[INFO|2025-01-15 13:59:52] llamafactory.train.tuner:157 >> Convert model dtype to: torch.bfloat16.\n",
            "[INFO|configuration_utils.py:414] 2025-01-15 13:59:52,530 >> Configuration saved in gemma_lora_merged/config.json\n",
            "[INFO|configuration_utils.py:865] 2025-01-15 13:59:52,530 >> Configuration saved in gemma_lora_merged/generation_config.json\n",
            "[INFO|modeling_utils.py:3043] 2025-01-15 14:01:41,174 >> The model is bigger than the maximum size per checkpoint (2GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at gemma_lora_merged/model.safetensors.index.json.\n",
            "[INFO|configuration_utils.py:414] 2025-01-15 14:01:42,054 >> Configuration saved in /tmp/tmp57ber8lp/config.json\n",
            "[INFO|configuration_utils.py:865] 2025-01-15 14:01:42,055 >> Configuration saved in /tmp/tmp57ber8lp/generation_config.json\n",
            "[INFO|modeling_utils.py:3043] 2025-01-15 14:05:34,157 >> The model is bigger than the maximum size per checkpoint (2GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /tmp/tmp57ber8lp/model.safetensors.index.json.\n",
            "[INFO|hub.py:817] 2025-01-15 14:06:02,941 >> Uploading the following files to mutoy/gemma-2b-if-finetuned-decode-kor: model-00002-of-00003.safetensors,model.safetensors.index.json,model-00001-of-00003.safetensors,generation_config.json,config.json,README.md,model-00003-of-00003.safetensors\n",
            "model-00002-of-00003.safetensors:   0% 0.00/1.98G [00:00<?, ?B/s]\n",
            "model-00001-of-00003.safetensors:   0% 0.00/1.95G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "Upload 3 LFS files:   0% 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   0% 1.51M/1.98G [00:00<02:28, 13.3MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   0% 16.4k/1.08G [00:00<2:38:05, 114kB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   0% 1.69M/1.95G [00:00<03:25, 9.48MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   0% 1.56M/1.08G [00:00<02:48, 6.40MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   0% 2.21M/1.08G [00:00<03:23, 5.30MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   0% 2.75M/1.08G [00:00<04:00, 4.49MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   0% 3.21M/1.08G [00:00<04:11, 4.29MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   0% 2.85M/1.98G [00:00<12:43, 2.59MB/s]\n",
            "model-00001-of-00003.safetensors:   0% 2.64M/1.95G [00:00<13:46, 2.36MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   0% 5.13M/1.98G [00:01<06:25, 5.13MB/s]\n",
            "model-00001-of-00003.safetensors:   0% 5.05M/1.95G [00:01<06:27, 5.01MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   0% 6.32M/1.98G [00:01<05:26, 6.05MB/s]\n",
            "model-00001-of-00003.safetensors:   0% 6.06M/1.95G [00:01<05:36, 5.76MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   0% 7.45M/1.98G [00:01<04:45, 6.91MB/s]\n",
            "model-00001-of-00003.safetensors:   0% 7.06M/1.95G [00:01<05:02, 6.42MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   0% 8.98M/1.98G [00:01<03:49, 8.59MB/s]\n",
            "model-00001-of-00003.safetensors:   0% 8.80M/1.95G [00:01<03:47, 8.51MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   1% 13.5M/1.98G [00:01<02:02, 16.0MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   1% 16.0M/1.98G [00:02<04:13, 7.76MB/s]\n",
            "model-00001-of-00003.safetensors:   1% 16.0M/1.95G [00:02<03:36, 8.92MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   1% 20.2M/1.98G [00:02<02:49, 11.6MB/s]\n",
            "model-00001-of-00003.safetensors:   1% 22.6M/1.95G [00:02<02:13, 14.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   1% 23.3M/1.98G [00:02<02:28, 13.2MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   2% 26.3M/1.08G [00:02<01:09, 15.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   1% 27.3M/1.98G [00:02<03:07, 10.4MB/s]\n",
            "model-00001-of-00003.safetensors:   1% 27.6M/1.95G [00:03<03:40, 8.70MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   3% 28.4M/1.08G [00:03<02:30, 7.00MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   1% 29.2M/1.95G [00:03<03:49, 8.36MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   3% 30.0M/1.08G [00:03<02:22, 7.38MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   2% 31.2M/1.95G [00:03<03:20, 9.55MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   2% 31.2M/1.98G [00:04<05:57, 5.46MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   3% 32.7M/1.08G [00:04<04:08, 4.23MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   4% 47.4M/1.08G [00:04<00:59, 17.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   2% 32.6M/1.95G [00:04<07:31, 4.24MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   2% 36.3M/1.95G [00:04<04:51, 6.55MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   2% 39.3M/1.95G [00:04<03:54, 8.13MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   2% 32.2M/1.98G [00:05<11:08, 2.92MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   2% 37.3M/1.98G [00:05<04:51, 6.67MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   2% 42.4M/1.98G [00:05<03:17, 9.81MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   6% 59.8M/1.08G [00:06<01:41, 10.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   2% 42.9M/1.95G [00:06<07:50, 4.05MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   2% 45.9M/1.98G [00:06<05:02, 6.40MB/s]\n",
            "model-00001-of-00003.safetensors:   2% 44.4M/1.95G [00:06<07:11, 4.41MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   2% 45.8M/1.95G [00:06<06:09, 5.15MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   2% 47.6M/1.95G [00:06<04:53, 6.47MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   6% 64.8M/1.08G [00:07<02:38, 6.42MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   2% 48.0M/1.98G [00:07<07:40, 4.20MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   7% 73.0M/1.08G [00:07<01:31, 11.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   3% 60.8M/1.98G [00:07<02:17, 14.0MB/s]\n",
            "model-00001-of-00003.safetensors:   3% 54.0M/1.95G [00:07<04:03, 7.78MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   3% 55.7M/1.95G [00:07<03:35, 8.76MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   3% 57.5M/1.95G [00:07<03:08, 10.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   4% 79.3M/1.98G [00:08<01:30, 20.9MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   7% 77.4M/1.08G [00:08<02:06, 7.95MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   7% 79.3M/1.08G [00:08<01:54, 8.77MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   4% 85.5M/1.98G [00:08<01:36, 19.7MB/s]\n",
            "model-00002-of-00003.safetensors:   5% 90.3M/1.98G [00:08<01:24, 22.3MB/s]\n",
            "model-00001-of-00003.safetensors:   3% 63.3M/1.95G [00:08<03:56, 7.96MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   7% 80.7M/1.08G [00:09<03:14, 5.14MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   9% 95.9M/1.08G [00:09<00:54, 18.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   3% 64.6M/1.95G [00:09<07:32, 4.17MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   9% 101M/1.08G [00:09<01:00, 16.2MB/s] \u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   4% 76.0M/1.95G [00:09<02:14, 13.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   5% 95.0M/1.98G [00:09<02:52, 10.9MB/s]\n",
            "model-00001-of-00003.safetensors:   4% 80.1M/1.95G [00:10<02:38, 11.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   4% 87.2M/1.95G [00:10<01:44, 17.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  10% 108M/1.08G [00:10<01:20, 12.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   5% 108M/1.98G [00:10<02:17, 13.6MB/s] \n",
            "model-00001-of-00003.safetensors:   5% 91.5M/1.95G [00:11<02:42, 11.4MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   5% 94.7M/1.95G [00:11<03:00, 10.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  10% 113M/1.08G [00:11<02:26, 6.62MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   6% 117M/1.98G [00:12<03:06, 9.98MB/s]\n",
            "model-00002-of-00003.safetensors:   6% 123M/1.98G [00:12<02:16, 13.6MB/s]\n",
            "model-00001-of-00003.safetensors:   6% 111M/1.95G [00:12<01:55, 15.9MB/s] \u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  11% 123M/1.08G [00:12<02:03, 7.74MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  12% 125M/1.08G [00:12<01:58, 8.07MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  12% 127M/1.08G [00:13<01:45, 9.08MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   6% 128M/1.98G [00:13<03:44, 8.25MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  12% 129M/1.08G [00:13<02:44, 5.79MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   7% 127M/1.95G [00:13<01:48, 16.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  13% 139M/1.08G [00:13<01:13, 12.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   7% 133M/1.95G [00:14<01:51, 16.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   7% 131M/1.98G [00:14<04:35, 6.72MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   7% 142M/1.98G [00:14<02:21, 13.0MB/s]\n",
            "model-00001-of-00003.safetensors:   7% 144M/1.95G [00:14<01:55, 15.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   8% 157M/1.98G [00:15<01:35, 19.2MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  14% 157M/1.08G [00:15<01:21, 11.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   9% 174M/1.98G [00:15<01:08, 26.3MB/s]\n",
            "model-00001-of-00003.safetensors:   8% 165M/1.95G [00:16<01:54, 15.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  10% 188M/1.98G [00:16<01:02, 28.6MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  15% 162M/1.08G [00:16<02:10, 7.03MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   9% 183M/1.95G [00:16<01:27, 20.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  10% 194M/1.98G [00:16<01:24, 21.3MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  10% 200M/1.98G [00:16<01:08, 26.0MB/s]\n",
            "model-00001-of-00003.safetensors:  10% 192M/1.95G [00:17<01:29, 19.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  11% 208M/1.95G [00:17<00:55, 31.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  10% 208M/1.98G [00:17<01:21, 21.8MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  11% 223M/1.98G [00:17<00:55, 32.0MB/s]\n",
            "model-00001-of-00003.safetensors:  11% 215M/1.95G [00:17<01:04, 26.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  18% 192M/1.08G [00:17<00:55, 16.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  19% 201M/1.08G [00:17<00:37, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  11% 224M/1.95G [00:18<01:15, 22.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  12% 236M/1.98G [00:18<01:16, 22.9MB/s]\n",
            "model-00001-of-00003.safetensors:  13% 247M/1.95G [00:18<00:58, 28.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  19% 206M/1.08G [00:19<01:17, 11.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  13% 249M/1.98G [00:19<01:39, 17.4MB/s]\n",
            "model-00002-of-00003.safetensors:  13% 261M/1.98G [00:19<01:23, 20.5MB/s]\n",
            "model-00002-of-00003.safetensors:  14% 269M/1.98G [00:20<00:59, 28.8MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  19% 209M/1.08G [00:20<01:50, 7.91MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  14% 280M/1.95G [00:20<01:00, 27.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  20% 212M/1.08G [00:20<01:34, 9.20MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  15% 287M/1.95G [00:20<00:50, 33.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  14% 275M/1.98G [00:20<01:18, 21.8MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  21% 224M/1.08G [00:20<01:00, 14.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  22% 239M/1.08G [00:20<00:30, 27.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  15% 293M/1.95G [00:20<01:18, 21.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  15% 303M/1.98G [00:21<00:45, 36.8MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  23% 245M/1.08G [00:21<00:40, 20.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  16% 310M/1.95G [00:21<01:08, 24.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  23% 253M/1.08G [00:21<00:31, 26.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  16% 310M/1.98G [00:21<00:58, 28.4MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  24% 258M/1.08G [00:21<00:38, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  16% 320M/1.95G [00:21<01:19, 20.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  24% 263M/1.08G [00:21<00:32, 25.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  16% 320M/1.98G [00:22<01:21, 20.4MB/s]\n",
            "model-00002-of-00003.safetensors:  17% 335M/1.98G [00:22<00:53, 31.0MB/s]\n",
            "model-00001-of-00003.safetensors:  18% 347M/1.95G [00:22<00:50, 31.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  25% 268M/1.08G [00:22<01:02, 13.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  18% 353M/1.95G [00:22<01:10, 22.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  25% 271M/1.08G [00:23<01:06, 12.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  17% 342M/1.98G [00:23<01:26, 18.9MB/s]\n",
            "model-00002-of-00003.safetensors:  18% 366M/1.98G [00:23<00:56, 28.8MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  25% 274M/1.08G [00:23<01:41, 7.93MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  20% 384M/1.95G [00:23<00:58, 26.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  20% 393M/1.95G [00:24<00:46, 33.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  19% 373M/1.98G [00:24<01:02, 25.6MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  19% 384M/1.98G [00:24<01:06, 24.0MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  20% 394M/1.98G [00:24<00:52, 30.5MB/s]\n",
            "model-00001-of-00003.safetensors:  21% 400M/1.95G [00:24<01:25, 18.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  21% 414M/1.95G [00:25<00:53, 28.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  28% 308M/1.08G [00:25<00:40, 19.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  21% 415M/1.98G [00:25<00:43, 36.3MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  30% 325M/1.08G [00:25<00:34, 22.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  21% 422M/1.98G [00:25<00:56, 27.4MB/s]\n",
            "model-00002-of-00003.safetensors:  22% 430M/1.98G [00:26<00:49, 31.5MB/s]\n",
            "model-00001-of-00003.safetensors:  22% 432M/1.95G [00:26<01:04, 23.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  22% 435M/1.98G [00:26<00:59, 26.0MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  22% 441M/1.98G [00:26<00:52, 29.6MB/s]\n",
            "model-00001-of-00003.safetensors:  22% 437M/1.95G [00:26<01:23, 18.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  23% 448M/1.95G [00:26<00:55, 27.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  23% 463M/1.98G [00:27<00:50, 29.9MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  33% 357M/1.08G [00:27<00:41, 17.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  24% 464M/1.95G [00:27<01:09, 21.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  24% 478M/1.98G [00:27<00:47, 32.0MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  34% 368M/1.08G [00:28<00:38, 18.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  35% 383M/1.08G [00:28<00:25, 27.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  25% 495M/1.98G [00:28<00:42, 34.7MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  36% 390M/1.08G [00:28<00:27, 24.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  25% 496M/1.95G [00:28<01:00, 24.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  26% 512M/1.98G [00:29<00:58, 25.3MB/s]\n",
            "model-00002-of-00003.safetensors:  26% 521M/1.98G [00:29<00:45, 31.8MB/s]\n",
            "model-00001-of-00003.safetensors:  27% 526M/1.95G [00:29<00:44, 32.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  27% 528M/1.98G [00:29<00:56, 25.9MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  38% 414M/1.08G [00:29<00:29, 22.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  27% 535M/1.98G [00:30<00:46, 31.4MB/s]\n",
            "model-00002-of-00003.safetensors:  27% 541M/1.98G [00:30<00:42, 33.9MB/s]\n",
            "model-00002-of-00003.safetensors:  28% 546M/1.98G [00:30<00:59, 24.2MB/s]\n",
            "model-00001-of-00003.safetensors:  28% 545M/1.95G [00:30<01:05, 21.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  28% 555M/1.98G [00:30<00:43, 33.0MB/s]\n",
            "model-00001-of-00003.safetensors:  28% 551M/1.95G [00:30<00:54, 25.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  39% 426M/1.08G [00:30<00:33, 19.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  28% 555M/1.95G [00:30<00:51, 27.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  29% 575M/1.98G [00:31<00:36, 39.0MB/s]\n",
            "model-00001-of-00003.safetensors:  29% 564M/1.95G [00:31<01:24, 16.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  40% 432M/1.08G [00:31<00:44, 14.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  29% 582M/1.98G [00:31<00:54, 25.9MB/s]\n",
            "model-00001-of-00003.safetensors:  30% 576M/1.95G [00:32<01:13, 18.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  41% 448M/1.08G [00:32<00:33, 18.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  30% 586M/1.95G [00:32<00:51, 26.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  31% 607M/1.98G [00:32<00:40, 34.0MB/s]\n",
            "model-00001-of-00003.safetensors:  30% 592M/1.95G [00:32<01:01, 22.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  31% 614M/1.98G [00:32<00:48, 28.4MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  43% 464M/1.08G [00:32<00:37, 16.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  44% 479M/1.08G [00:33<00:21, 27.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  32% 614M/1.95G [00:33<00:48, 27.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  45% 486M/1.08G [00:33<00:25, 23.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  32% 624M/1.95G [00:33<00:56, 23.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  32% 639M/1.98G [00:34<00:50, 26.6MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  46% 496M/1.08G [00:34<00:26, 21.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  47% 511M/1.08G [00:34<00:17, 33.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  34% 671M/1.98G [00:35<00:40, 32.6MB/s]\n",
            "model-00001-of-00003.safetensors:  34% 656M/1.95G [00:35<01:05, 19.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  34% 670M/1.95G [00:35<00:44, 28.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  34% 678M/1.98G [00:35<00:50, 25.6MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  49% 528M/1.08G [00:35<00:30, 18.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  35% 676M/1.95G [00:35<01:01, 20.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  50% 543M/1.08G [00:35<00:19, 27.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  35% 703M/1.98G [00:36<00:37, 34.2MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  51% 550M/1.08G [00:36<00:23, 22.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  35% 688M/1.95G [00:36<01:03, 19.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  36% 695M/1.95G [00:36<00:49, 25.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  52% 560M/1.08G [00:36<00:24, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  36% 711M/1.98G [00:37<01:05, 19.4MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  36% 720M/1.98G [00:37<01:04, 19.6MB/s]\n",
            "model-00002-of-00003.safetensors:  37% 735M/1.98G [00:37<00:42, 29.5MB/s]\n",
            "model-00001-of-00003.safetensors:  36% 709M/1.95G [00:37<01:16, 16.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  55% 592M/1.08G [00:37<00:19, 24.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  37% 742M/1.98G [00:38<00:46, 26.6MB/s]\n",
            "model-00001-of-00003.safetensors:  37% 720M/1.95G [00:38<01:05, 18.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  38% 735M/1.95G [00:38<00:40, 30.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  39% 767M/1.98G [00:38<00:35, 34.7MB/s]\n",
            "model-00001-of-00003.safetensors:  38% 742M/1.95G [00:39<00:52, 22.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  58% 624M/1.08G [00:39<00:18, 24.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  39% 774M/1.98G [00:39<00:42, 28.3MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  59% 640M/1.08G [00:39<00:17, 25.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  39% 752M/1.95G [00:39<01:02, 19.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  60% 654M/1.08G [00:39<00:11, 38.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  40% 793M/1.98G [00:39<00:39, 29.8MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  61% 662M/1.08G [00:40<00:13, 31.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  39% 768M/1.95G [00:40<00:57, 20.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  40% 783M/1.95G [00:40<00:35, 32.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  62% 672M/1.08G [00:40<00:15, 25.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  41% 814M/1.98G [00:40<00:40, 28.6MB/s]\n",
            "model-00001-of-00003.safetensors:  41% 791M/1.95G [00:41<00:52, 21.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  41% 821M/1.98G [00:41<00:50, 23.1MB/s]\n",
            "model-00001-of-00003.safetensors:  41% 800M/1.95G [00:41<00:55, 20.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  42% 814M/1.95G [00:41<00:36, 30.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  65% 704M/1.08G [00:41<00:15, 24.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  43% 847M/1.98G [00:42<00:35, 32.2MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  67% 725M/1.08G [00:42<00:12, 29.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  43% 855M/1.98G [00:42<00:44, 25.3MB/s]\n",
            "model-00002-of-00003.safetensors:  44% 863M/1.98G [00:42<00:37, 30.1MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  68% 736M/1.08G [00:42<00:13, 25.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  69% 751M/1.08G [00:42<00:08, 37.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  44% 869M/1.98G [00:43<00:47, 23.6MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  70% 759M/1.08G [00:43<00:11, 28.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  44% 848M/1.95G [00:43<00:51, 21.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  45% 892M/1.98G [00:43<00:34, 31.9MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  71% 768M/1.08G [00:43<00:13, 24.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  45% 898M/1.98G [00:44<00:44, 24.6MB/s]\n",
            "model-00002-of-00003.safetensors:  46% 905M/1.98G [00:44<00:37, 29.0MB/s]\n",
            "model-00002-of-00003.safetensors:  46% 912M/1.98G [00:44<00:32, 33.4MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  46% 917M/1.98G [00:44<00:40, 26.1MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  74% 800M/1.08G [00:45<00:12, 22.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  75% 815M/1.08G [00:45<00:07, 33.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  47% 928M/1.98G [00:45<00:44, 23.5MB/s]\n",
            "model-00002-of-00003.safetensors:  47% 937M/1.98G [00:45<00:34, 30.5MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  76% 822M/1.08G [00:45<00:09, 27.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  46% 899M/1.95G [00:45<00:54, 19.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  48% 959M/1.98G [00:46<00:27, 36.9MB/s]\n",
            "model-00002-of-00003.safetensors:  49% 966M/1.98G [00:46<00:37, 26.8MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  77% 832M/1.08G [00:46<00:14, 17.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  78% 847M/1.08G [00:46<00:08, 26.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  48% 928M/1.95G [00:46<00:46, 22.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  50% 992M/1.98G [00:47<00:26, 36.9MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  79% 854M/1.08G [00:47<00:09, 24.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  50% 999M/1.98G [00:47<00:35, 27.5MB/s]\n",
            "model-00001-of-00003.safetensors:  49% 959M/1.95G [00:47<00:32, 30.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  51% 1.01G/1.98G [00:47<00:31, 31.2MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  81% 872M/1.08G [00:47<00:07, 26.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  50% 965M/1.95G [00:48<00:40, 24.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  51% 1.01G/1.98G [00:48<00:46, 20.9MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  51% 1.02G/1.98G [00:48<00:35, 27.4MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  82% 893M/1.08G [00:48<00:06, 30.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  50% 976M/1.95G [00:48<00:43, 22.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  52% 1.04G/1.98G [00:49<00:27, 34.8MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  83% 898M/1.08G [00:49<00:09, 19.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  51% 998M/1.95G [00:49<00:35, 26.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  84% 910M/1.08G [00:49<00:05, 30.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  53% 1.05G/1.98G [00:49<00:41, 22.4MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  85% 916M/1.08G [00:49<00:07, 21.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  52% 1.01G/1.95G [00:49<00:41, 22.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  85% 923M/1.08G [00:49<00:05, 27.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  54% 1.07G/1.98G [00:50<00:29, 30.6MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  86% 928M/1.08G [00:50<00:07, 20.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  53% 1.02G/1.95G [00:50<00:41, 22.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  87% 938M/1.08G [00:50<00:04, 29.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  53% 1.03G/1.95G [00:50<00:33, 27.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  53% 1.04G/1.95G [00:50<00:26, 33.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  55% 1.08G/1.98G [00:51<00:32, 27.8MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  88% 953M/1.08G [00:51<00:04, 31.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  54% 1.04G/1.95G [00:51<00:35, 25.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  89% 959M/1.08G [00:51<00:03, 33.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  54% 1.05G/1.95G [00:51<00:34, 26.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  56% 1.10G/1.98G [00:51<00:26, 33.7MB/s]\n",
            "model-00001-of-00003.safetensors:  54% 1.06G/1.95G [00:51<00:37, 23.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  55% 1.07G/1.95G [00:51<00:26, 33.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  56% 1.11G/1.98G [00:52<00:33, 25.7MB/s]\n",
            "model-00002-of-00003.safetensors:  56% 1.12G/1.98G [00:52<00:27, 31.3MB/s]\n",
            "model-00001-of-00003.safetensors:  55% 1.08G/1.95G [00:52<00:29, 29.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  90% 976M/1.08G [00:52<00:05, 18.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  92% 991M/1.08G [00:52<00:02, 30.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  56% 1.09G/1.95G [00:52<00:39, 21.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  57% 1.10G/1.95G [00:52<00:22, 36.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  57% 1.12G/1.98G [00:53<00:51, 16.5MB/s]\n",
            "model-00002-of-00003.safetensors:  58% 1.15G/1.98G [00:53<00:28, 29.2MB/s]\n",
            "model-00001-of-00003.safetensors:  57% 1.12G/1.95G [00:54<00:36, 22.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  93% 1.01G/1.08G [00:54<00:04, 17.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  58% 1.13G/1.95G [00:54<00:26, 30.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  58% 1.16G/1.98G [00:54<00:33, 24.3MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  59% 1.16G/1.98G [00:54<00:30, 26.7MB/s]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  95% 1.03G/1.08G [00:54<00:02, 20.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  58% 1.14G/1.95G [00:54<00:41, 19.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  60% 1.18G/1.98G [00:55<00:28, 28.2MB/s]\n",
            "model-00001-of-00003.safetensors:  59% 1.16G/1.95G [00:55<00:31, 25.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  96% 1.04G/1.08G [00:55<00:02, 17.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  60% 1.20G/1.98G [00:55<00:26, 29.9MB/s]\n",
            "model-00001-of-00003.safetensors:  60% 1.17G/1.95G [00:55<00:34, 22.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  98% 1.06G/1.08G [00:56<00:01, 20.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  61% 1.18G/1.95G [00:56<00:22, 34.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  61% 1.21G/1.98G [00:56<00:32, 23.7MB/s]\n",
            "model-00001-of-00003.safetensors:  61% 1.19G/1.95G [00:56<00:27, 27.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  62% 1.23G/1.98G [00:56<00:21, 34.8MB/s]\n",
            "model-00001-of-00003.safetensors:  62% 1.20G/1.95G [00:56<00:28, 25.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors: 100% 1.08G/1.08G [00:57<00:00, 18.9MB/s]\n",
            "\n",
            "model-00002-of-00003.safetensors:  64% 1.26G/1.98G [00:57<00:20, 35.7MB/s]\n",
            "model-00001-of-00003.safetensors:  63% 1.23G/1.95G [00:58<00:30, 23.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  64% 1.27G/1.98G [00:58<00:25, 27.8MB/s]\n",
            "model-00002-of-00003.safetensors:  65% 1.29G/1.98G [00:59<00:19, 34.9MB/s]\n",
            "model-00001-of-00003.safetensors:  65% 1.26G/1.95G [00:59<00:28, 24.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  66% 1.30G/1.98G [00:59<00:24, 27.7MB/s]\n",
            "model-00002-of-00003.safetensors:  66% 1.31G/1.98G [01:00<00:28, 23.1MB/s]\n",
            "model-00002-of-00003.safetensors:  67% 1.33G/1.98G [01:00<00:19, 33.6MB/s]\n",
            "model-00001-of-00003.safetensors:  67% 1.30G/1.95G [01:00<00:21, 30.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  67% 1.31G/1.95G [01:00<00:25, 24.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  67% 1.33G/1.98G [01:00<00:29, 22.0MB/s]\n",
            "model-00002-of-00003.safetensors:  69% 1.36G/1.98G [01:01<00:21, 28.9MB/s]\n",
            "model-00001-of-00003.safetensors:  69% 1.34G/1.95G [01:01<00:25, 23.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  69% 1.37G/1.98G [01:02<00:24, 24.8MB/s]\n",
            "model-00002-of-00003.safetensors:  70% 1.39G/1.98G [01:02<00:16, 35.6MB/s]\n",
            "model-00001-of-00003.safetensors:  71% 1.38G/1.95G [01:02<00:22, 25.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  71% 1.40G/1.98G [01:03<00:19, 29.3MB/s]\n",
            "model-00002-of-00003.safetensors:  72% 1.43G/1.98G [01:04<00:19, 28.0MB/s]\n",
            "model-00001-of-00003.safetensors:  72% 1.41G/1.95G [01:04<00:28, 19.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  73% 1.45G/1.98G [01:04<00:14, 35.8MB/s]\n",
            "model-00002-of-00003.safetensors:  74% 1.46G/1.98G [01:05<00:18, 28.6MB/s]\n",
            "model-00001-of-00003.safetensors:  74% 1.44G/1.95G [01:05<00:22, 22.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  75% 1.49G/1.98G [01:06<00:13, 36.9MB/s]\n",
            "model-00002-of-00003.safetensors:  75% 1.49G/1.98G [01:06<00:17, 28.5MB/s]\n",
            "model-00001-of-00003.safetensors:  76% 1.47G/1.95G [01:06<00:19, 24.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  77% 1.52G/1.98G [01:07<00:12, 38.5MB/s]\n",
            "model-00002-of-00003.safetensors:  77% 1.53G/1.98G [01:07<00:15, 28.9MB/s]\n",
            "model-00001-of-00003.safetensors:  77% 1.50G/1.95G [01:07<00:18, 24.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  78% 1.55G/1.98G [01:08<00:12, 35.4MB/s]\n",
            "model-00002-of-00003.safetensors:  79% 1.56G/1.98G [01:08<00:15, 28.0MB/s]\n",
            "model-00001-of-00003.safetensors:  79% 1.54G/1.95G [01:08<00:17, 23.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  80% 1.58G/1.98G [01:09<00:10, 38.3MB/s]\n",
            "model-00002-of-00003.safetensors:  80% 1.59G/1.98G [01:09<00:13, 29.9MB/s]\n",
            "model-00001-of-00003.safetensors:  80% 1.57G/1.95G [01:10<00:16, 23.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  81% 1.61G/1.98G [01:10<00:10, 34.8MB/s]\n",
            "model-00001-of-00003.safetensors:  82% 1.59G/1.95G [01:10<00:16, 22.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  82% 1.60G/1.95G [01:11<00:16, 21.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  83% 1.62G/1.95G [01:11<00:10, 31.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  83% 1.62G/1.95G [01:12<00:12, 26.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  84% 1.63G/1.95G [01:12<00:17, 18.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  82% 1.62G/1.98G [01:13<00:40, 9.01MB/s]\n",
            "model-00002-of-00003.safetensors:  83% 1.65G/1.98G [01:13<00:18, 17.7MB/s]\n",
            "model-00001-of-00003.safetensors:  85% 1.66G/1.95G [01:13<00:12, 23.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  83% 1.65G/1.98G [01:14<00:19, 17.2MB/s]\n",
            "model-00002-of-00003.safetensors:  85% 1.68G/1.98G [01:14<00:11, 25.6MB/s]\n",
            "model-00001-of-00003.safetensors:  87% 1.70G/1.95G [01:15<00:10, 24.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  85% 1.68G/1.98G [01:15<00:13, 22.5MB/s]\n",
            "model-00002-of-00003.safetensors:  86% 1.71G/1.98G [01:16<00:08, 31.2MB/s]\n",
            "model-00001-of-00003.safetensors:  89% 1.73G/1.95G [01:16<00:09, 24.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  87% 1.72G/1.98G [01:16<00:09, 27.1MB/s]\n",
            "model-00002-of-00003.safetensors:  88% 1.74G/1.98G [01:17<00:06, 35.5MB/s]\n",
            "model-00001-of-00003.safetensors:  90% 1.76G/1.95G [01:17<00:07, 26.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  88% 1.75G/1.98G [01:17<00:08, 26.6MB/s]\n",
            "model-00002-of-00003.safetensors:  89% 1.76G/1.98G [01:17<00:06, 33.4MB/s]\n",
            "model-00002-of-00003.safetensors:  89% 1.77G/1.98G [01:18<00:07, 27.4MB/s]\n",
            "model-00001-of-00003.safetensors:  92% 1.80G/1.95G [01:18<00:05, 26.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  90% 1.79G/1.98G [01:18<00:05, 35.8MB/s]\n",
            "model-00001-of-00003.safetensors:  93% 1.81G/1.95G [01:18<00:04, 28.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  94% 1.82G/1.95G [01:18<00:03, 36.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  94% 1.83G/1.95G [01:19<00:04, 26.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  94% 1.84G/1.95G [01:19<00:04, 21.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  91% 1.80G/1.98G [01:20<00:14, 12.5MB/s]\n",
            "model-00001-of-00003.safetensors:  96% 1.86G/1.95G [01:20<00:03, 22.4MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  96% 1.87G/1.95G [01:20<00:02, 29.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  92% 1.82G/1.98G [01:21<00:08, 19.1MB/s]\n",
            "model-00001-of-00003.safetensors:  97% 1.89G/1.95G [01:21<00:02, 22.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  92% 1.83G/1.98G [01:21<00:08, 18.4MB/s]\n",
            "model-00002-of-00003.safetensors:  93% 1.84G/1.98G [01:22<00:07, 19.1MB/s]\n",
            "model-00002-of-00003.safetensors:  93% 1.85G/1.98G [01:22<00:04, 26.6MB/s]\n",
            "model-00002-of-00003.safetensors:  94% 1.86G/1.98G [01:23<00:05, 22.5MB/s]\n",
            "model-00002-of-00003.safetensors:  94% 1.87G/1.98G [01:23<00:04, 26.6MB/s]\n",
            "model-00001-of-00003.safetensors: 100% 1.95G/1.95G [01:23<00:00, 23.2MB/s]\n",
            "model-00002-of-00003.safetensors: 100% 1.98G/1.98G [01:31<00:00, 21.6MB/s]\n",
            "\n",
            "\n",
            "Upload 3 LFS files: 100% 3/3 [01:32<00:00, 30.72s/it]\n",
            "[INFO|tokenization_utils_base.py:2646] 2025-01-15 14:07:37,482 >> tokenizer config file saved in gemma_lora_merged/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2655] 2025-01-15 14:07:37,482 >> Special tokens file saved in gemma_lora_merged/special_tokens_map.json\n",
            "README.md: 100% 5.19k/5.19k [00:00<00:00, 20.8MB/s]\n",
            "[INFO|tokenization_utils_base.py:2646] 2025-01-15 14:07:38,913 >> tokenizer config file saved in /tmp/tmpwzjn2pfw/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2655] 2025-01-15 14:07:38,914 >> Special tokens file saved in /tmp/tmpwzjn2pfw/special_tokens_map.json\n",
            "[INFO|hub.py:817] 2025-01-15 14:07:39,567 >> Uploading the following files to mutoy/gemma-2b-if-finetuned-decode-kor: tokenizer.model,tokenizer.json,README.md,special_tokens_map.json,tokenizer_config.json\n",
            "tokenizer.model:   0% 0.00/4.24M [00:00<?, ?B/s]\n",
            "Upload 2 LFS files:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "tokenizer.json:   0% 0.00/34.4M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "tokenizer.model: 100% 4.24M/4.24M [00:00<00:00, 8.56MB/s]\n",
            "\n",
            "\n",
            "tokenizer.json:  57% 19.4M/34.4M [00:00<00:00, 29.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "tokenizer.json:  92% 31.5M/34.4M [00:00<00:00, 48.3MB/s]\u001b[A\u001b[A\n",
            "tokenizer.json: 100% 34.4M/34.4M [00:01<00:00, 22.8MB/s]\n",
            "\n",
            "Upload 2 LFS files: 100% 2/2 [00:01<00:00,  1.05it/s]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "    model_name_or_path=\"google/gemma-2b-it\",  # use official non-quantized Gemma 2B model\n",
        "    adapter_name_or_path=\"gemma_lora\",  # load the saved LoRA adapters\n",
        "    template=\"gemma\",  # same to the one in training\n",
        "    finetuning_type=\"lora\",  # same to the one in training\n",
        "    export_dir=\"gemma_lora_merged\",  # path to save the merged model\n",
        "    export_size=2,  # the file shard size (in GB) of the merged model\n",
        "    export_device=\"cpu\",  # the device used in export, can be chosen from `cpu` and `cuda`\n",
        "    export_hub_model_id=\"gemma-2b-if-finetuned-decode-kor\",  # your Hugging Face hub model ID\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"merge_gemma.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli export merge_gemma.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pWUj7iJnO9D1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KCThY-zIRJ7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a4_yNybC5zJ5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Finetune_with_LLaMA_Factory.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "55ce251259d1469ba547ec8d662028af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_40abfe1f857f4b09b7283db8f51936c3",
              "IPY_MODEL_9f72082ca58d40008a34fa905339ff32",
              "IPY_MODEL_751559bac92a493bbd199ac950f4b0d7"
            ],
            "layout": "IPY_MODEL_2e0506749318480a8ba784c4135a7432"
          }
        },
        "40abfe1f857f4b09b7283db8f51936c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27d5b3e787774fed95d45ffa56adc1cb",
            "placeholder": "​",
            "style": "IPY_MODEL_b377c06e79b240b5bd94b7257624786f",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "9f72082ca58d40008a34fa905339ff32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30600970fd644933adb3759cd67c139d",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_80b133558ba647c890693b75839b3a18",
            "value": 2
          }
        },
        "751559bac92a493bbd199ac950f4b0d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b51dff014024503bd07a6b5a45935ff",
            "placeholder": "​",
            "style": "IPY_MODEL_128f5c72dce2480eb73f525cfac98839",
            "value": " 2/2 [00:02&lt;00:00,  1.20s/it]"
          }
        },
        "2e0506749318480a8ba784c4135a7432": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27d5b3e787774fed95d45ffa56adc1cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b377c06e79b240b5bd94b7257624786f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30600970fd644933adb3759cd67c139d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80b133558ba647c890693b75839b3a18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6b51dff014024503bd07a6b5a45935ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "128f5c72dce2480eb73f525cfac98839": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}